---
layout: post
title:  "StarGAN 정리"
date:   2018-12-08 17:17:00 +0900
lang: ko
tags: Deeplearning GAN
---
# StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-toImage Translation #

##### Choi, Yunjey, et al. "Stargan: Unified generative adversarial networks for multi-domain image-to-image translation." arXiv preprint 1711 (2017). #####
<hr>

## 1. Introduction ##
&nbsp;&nbsp;image-to-image translation은 한 이미지에서 다른 이미지로 바꾸는 기술이다. 특히 Generative Adversarial Networks(GANs)로 image-to-image translation의 기술을 크게 발전시켰다.
<br>
<br>
두 개의 서로 다른 **domain**으로부터 training data가 주어졌을 때 model은 한 domain에서 다른 domain으로 image translation을 하도록 학습한다. 이 때의 term들을 다음과 같이 정의한다.
* attribute : meaningful feature(hair color, gender, age..)
* attribute value : a particular value of an attribute(brown, black, male, female..)
* domain : a set of images sharing the same attribute
<br>

![]({{ site.url }}/assets/img/StarGAN/01.png)

특히 몇몇 image datasets은 수많은 labeled attributes와 관련이 있다.
* CelebA dataset : facial attributes(머리색, 성별, 나이 등..)와 관련된 40개의 label들을 가지고 있다.
* RaFD dataset : facial expressions(happy, angry, sad)와 같은 8개의 label들을 가지고 있다.

StarGAN은 이러한 점에서 multiple domain으로부터 attribute에 따라 image를 바꾸는 multi-domain image-to-image translation을 생각해낸 것이다. Figure 1의 우측은 RaFD를 학습하면서 얻은 feature를 이용하여 CelebA와 RaFD jointly training으로 CelebA image의 표정을 바꾸는 학습을 진행한다.

![]({{ site.url }}/assets/img/StarGAN/02.png){: width="50%" height="50%" .center}

&nbsp;&nbsp;그러나 기존의 모델은 multi-domain image translation에 매우 비효율적이다. 그 이유는 다음 3가지와 같다.
* k개의 domain 간 mapping을 학습할 때 (k-1)개의 generator를 학습해야 하기 때문이다.
* 또한 각각의 generator들은 전체 training data를 완전히 사용하지 못하고 k개의 domain 중에 2개만 학습한다. 이는 생성되는 이미지의 **품질 저하**를 일으킨다. 
* 게다가 기존의 모델에서는 서로 다른 dataset으로부터 jointly training domain이 불가능하다. 왜냐하면 각각의 dataset은 *partially labeled*이기 때문에..<a href="#sec3_2">Section 3.2</a>
<br>
<br>
![]({{ site.url }}/assets/img/StarGAN/03.png){: width="50%" height="50%" .center}

&nbsp;&nbsp;StarGAN은 위의 그림처럼 하나의 generator로 여러 multiple domain 사이의 mapping을 학습시키는 모델 구조를 제안한다.
<br>
또한 input으로는 **image**와 **domain information**을 넣는다. 이 때 domain information은 label(binary나 one-hot vector)을 사용한다.<br><br>
그리고 서로 다른 dataset의 domain의 joint training을 위해 domain label에 **mask vector**라는 정보를 추가한다. 이 방법은 알려지지 않는 label은 **무시하고** 특정 dataset의 label에만 **집중** 할 수 있게 된다.

따라서 요악하면 다음과 같다.
* 각각의 domain의 image로부터 효율적으로 image-to-image translation을 학습하기 위해 **single generator와 discriminator**을 이용하여 multiple domains의 mapping을 학습하는 starGAN 제안
* domain labels를 control하기 위해 **mask vector**를 사용하여 multi domain image translation 학습
* StarGAN을 이용하여 facial attribute transfer와 facial expression synthesis에서 좋은 성과를 얻음

<br><br>
## 2. Related Work ##
#### <b>Generative Adversarial Networks</b> ####
생성되는 이미지가 더욱 실감나도록 adversarial loss를 leverage

#### <b>Conditional GANs</b> ####
본 논문에서는 conditional domain information을 넣기 위해 **scalabe GAN framework**를 사용한다. 이는 다양한 domain에서 image translation을 유연하게 control할 수 있도록 한다.

#### <b>Image-to-Image Translation</b> ####
CycleGAN과 DiscoGAN은 **cycle consistency loss**를 활용하여 input과 translated image 사이의 key attributes를 보존하므로 즉, 복원된 이미지가 입력 이미지와 비슷하게 학습시키기 위해 starGAN에서도 cycle consistency loss를 추가한다.




<br><br>
## 3. Star Generative Adversarial Networks ##
이번 섹션에서는 서로 다른 label을 갖는 multiple dataset을 가지고 StarGAN으로 label을 통해 image translation 시키는 지 알아본다.
![]({{ site.url }}/assets/img/StarGAN/04.png){: .center}

### 3.1. Multi-Domain Image-to-Image Translation ###
&nbsp;&nbsp;먼저 Generator를 학습시키기 위해서<br>
input: x, target domain label: c, output: y<br>
G(x, c) → y<br><br>
또한 auxiliary classifier을 통해 하나의 discriminator가 multiple domain을 control할 수 있도록 하였다. Discrimintor은 source와 domain label의 **probability distributions**을 생성한다.<br>
D : x → {D<sub>src</sub>(x), D<sub>cls</sub>(x)}

#### <b>Adversarial Loss</b> ####
&nbsp;&nbsp;생성된 이미지와 원본 이미지를 구별하기 위해 adversarial loss를 사용한다.
![]({{ site.url }}/assets/img/StarGAN/05.png){: width="60%" height="60%" .center}

여기서 D<sub>src</sub>(x)는 discriminator D에 의해 생성된 source의 probability distribution을 뜻한다.<br>
Generator G는 최소화되고 discriminator D는 최대화하도록 할 것이다.

#### <b>Domain Classification Loss</b> ####
&nbsp;&nbsp;input x와 target domain label c가 주어졌을 때 우리는 x에서 target domain c로 적절히 분리 된 output y를 만드는 것이다. 이를 위해서 D의 상단에 **auxiliary classifier**을 추가하고 D와 G를 최적화 할 때 **domain classification loss**를 추가한다. 정리하면 다음과 같다. <br>
* 실제 이미지의 domain classification loss → D를 최적화
* 가짜 이미지의 domain classification loss → G를 최적화

![]({{ site.url }}/assets/img/StarGAN/06.png){: width="60%" height="60%" .center}

여기서 D<sub>cls</sub>(c'ㅣx)는 D에 의해 계산된 domain label의 probability distribution이다. 따라서 D는 결국 이 classification loss를 최소화시켜, original domain c'에 해당하는 real image를 분류할 수 있다.<br><br>

반면에 fake image의 domain classification에 대한 loss funcion은 다음과 같다.
![]({{ site.url }}/assets/img/StarGAN/07.png){: width="60%" height="60%" .center}
여기서 G는 위 식의 loss를 줄여나가며 target domain c에 분류되는 이미지를 생성하도록 한다.

#### <b>Reconstruction Loss</b> ####
&nbsp;&nbsp;위의 설명했던 adversarial loss와 classification losses를 최소화함으로써 G는 더 현실적인 이미지를 생성하도록 학습하고 target domain에 정확히 대응되도록 분류한다. <br>
"However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs."<br>
하지만 loss들을 최소화하면 바뀐 이미지가 input과 관련된 domain을 바꾸는 동안 입력 이미지의 내용을 유지한다는 보장이 없다.(?)<br>
따라서 generator에 **cycle consistency loss**를 적용시켜 문제를 해결하고자 했다. 이는 다음과 같다.
![]({{ site.url }}/assets/img/StarGAN/08.png){: width="60%" height="60%" .center}

여기서 G는 변환이미지 G(x,c)와 original domain label c'를 input으로 다시 original image x를 **재구축(reconstruction)**하게 되며 L1 norm을 reconstruction loss로 사용하였다. 이 때 주목할 점은 *single generator을 두 번 사용한다*
<br><br>
그렇다면 이제 다음 그림을 보면서 StarGAN의 전반적인 학습 프로세스를 요악한다. 
<br>
![]({{ site.url }}/assets/img/StarGAN/14.png){: .center}
![]({{ site.url }}/assets/img/StarGAN/15.png){: .center}

* G(x:input image, c:target domain) → y:Fake image
* G(y:Fake image, c':original domain) → x:reconstructed image
* D : Distinguish between real & fake + auxiliary classifier(domain label classification)

<br>
#### <b>Full Objective</b> ####
&nbsp;&nbsp;G와 D를 최적화하는 objective functions는 다음과 같다.
![]({{ site.url }}/assets/img/StarGAN/09.png){: width="60%" height="60%" .center}

<p id="sec3_2">
</p>
<br>

### 3.2. Training with Multiple Datasets ###

&nbsp;&nbsp;StarGAN의 중요한 장점은 전혀 다른 label을 포함한 multiple dataset을 동시에 적용시켜 모든 label을 control할 수 있다는 점이다. 앞서 설명드린 서로 다른 label을 갖는 CelebA와 RaFD의 경우다. 예를 들어 CelebA는 attribute를 갖지만 facial expression label은 갖고 있지 않다. 이를 해결하기 위해서 reconstruction process의 <b>label vector c'</b>를 추가할 것이다..
<br>
#### <b>Mask Vector</b> ####
StarGAN은 mask vector m을 통해서 필요없는 label은 무시하고 특정 dataset의 label에 집중하도록 도와준다. mask vector m을 나타내기 위해서 datasets의 수(data의 수가 아님) n개가 있다면 **n-dimensional one-hot vector**을 구성한다. 
![]({{ site.url }}/assets/img/StarGAN/10.png){: width="40%" height="40%" .center}<br>
C<sub>i</sub>는 i번째 dataset의 label을 나타내는 vector이다. label c<sub>i</sub>를 표현하는 방법은 binary attribute의 **binary vector** 혹은 categorical attribute을 나타내는 **one-hot vector**일 수 있다. 이 때 나머지 n-1개의 unknown labels는 모두 0으로 준다.

#### <b>Training Strategy</b> ####
&nbsp;&nbsp;위에 설명드렸던 domain label vector c ̃를 generator의 input으로 넣을 것이다. 그러면 generator은 불필요한 label(zero vector)을 무시하고 주어진 label을 더욱 집중할 수 있을 것이다..<br>
<b>
"By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label.
"
</b>
* G의 구조는 input label c ̃의 차원이 아닌 single dataset 학습 할 때와 동일하다.
* 모든 dataset에 대한 label에 대해 probability distribution을 만들기 위해 D의 auxiliary classfier를 확장한다.
* D가 인식 된 label과 관련된 classification error만을 최소화시키기 위해 multi-task에서 학습을 수행한다.
<br><br>

## 4. Implementation ##
#### <b>Improved GAN Training</b> ####
&nbsp;&nbsp;학습을 안정화시키고 high quality image를 만들기 위해 Adversarial loss를 gradient penalty가 포함 된 **Wasserstein GAN objective**로 바꾼다.
![]({{ site.url }}/assets/img/StarGAN/05.png){: width="60%" height="60%" .center}
<center>↓</center><br>
![]({{ site.url }}/assets/img/StarGAN/11.png){: width="60%" height="60%" .center}<br>
λ<sub>gp</sub> = 10으로 설정한다. x^는 한 pair의 real image와 생성된 이미지 사이의 직선을 따라 균일하게 샘플링 된다.(?)<br>

#### <b>Network Architecture</b> ####
&nbsp;&nbsp;StarGaN의 generator network와 discriminator는 다음과 같이 구성되어 있다.
* Generator network architecture
![]({{ site.url }}/assets/img/StarGAN/12.png){: width="80%" height="80%" .center}<br>
* Discriminator network architecture
![]({{ site.url }}/assets/img/StarGAN/13.png){: width="80%" height="80%" .center}
<br><br>

## 5. Experiments ##

### 5.1. Baseline Models ###
* DIAT
* CycleGAN
* IcGAN

### 5.2. Dataset ###
* CelebA : 202,599 face images of celebrities, 40 binary attributes, 7 domains
    + Attributes : hair color(black, blond, brown), gender(male/female), age(young/old)
* RaFD : 4,824 images collected from 67 participants, 8 facial expression in 3 different gaze directions

### 5.3. Training ###
* Using Adam optimizer, β<sub>1</sub> = 0.5, β<sub>2</sub> = 0.999
* Batch size : 16
* Learning rate
    + CelebA : 0.0001에서 10 epochs마다 감소
    + RaFD : 0.0001에서 100 epochs 마다 감소
* Source : single NVIDIA Tesla M40 GPU

### 5.4. Experimental Results on CelebA ###

![]({{ site.url }}/assets/img/StarGAN/16.png){: .center}
&nbsp;&nbsp;먼저 CelebA로만 학습시켰을 때의 결과다. multiple attribute에서 합성을 진행하였으며, 좋은 퀄리티를 낼 수 있었다.

### 5.5. Experimental Results on RaFD ###
![]({{ site.url }}/assets/img/StarGAN/17.png){: .center}
&nbsp;&nbsp;다음은 RaFD로만 학습을 시켰을 때의 결과이다.

### 5.6. Experimental Results on CelebA + RaFD ###
![]({{ site.url }}/assets/img/StarGAN/18.png){: .center}
&nbsp;&nbsp;Multi Domatin뿐만 아니라 Multi dataset으로도 학습시킨 결과이다.
* StarGAN(SNG) : RaFD로 학습시킨 모델로 CelebA에 적용시킨 결과
* StarGAN(JNT) : RaFD + CelebA로 학습시킨 모델로 CelebA에 적용시킨 결과

multi dataset으로 학습시킨 모델이 조금 더 사진을 잘 생성해내는 것을 알 수 있다.


