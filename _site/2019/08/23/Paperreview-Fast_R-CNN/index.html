<!DOCTYPE html>
<html lang="ko">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>[Paper review] Fast R-CNN</title>
  <meta name="description" content="Fast R-CNN 1) Introduction   전에 리뷰한 R-CNN에는 3가지에 문제가 있다. 3단계 모델을 거친 pipeline 구조 Region-proposal을 추출하기 위해 Selective search 알고리즘, feature vector을 얻기 위한 CNN, 그리고 feature vector로 분류하는 SVM classifier, 더 정확한 bounding-box를 얻기위한 regressor Training에 자원과 시간이 많이 소모 각각 이미지에서 얻은 2000개의 region-proposal로부터 CNN을 통해 얻은 feature vector로 SVM과 bounding-box regressor를 훈련할 때 디스크에 이 data들을 모두 쓰면서 deep network를 training한다. 느린 수행 속도 역시나 각각의 test image에서 region proposal, feature vector을 얻는 데 시간이 걸린다. 원치않는 이미지의 왜곡과 손실 CNN의 input을 맞추기 위해 warping 및 cropping이 일어나 정보의 손실 발생 앞서 b-box 사이에 겹치는 영역을 따로따로 CNN에 통과시키는 것은 비용적으로 낭비이다. 그래서 제안된 것이 SPPnets이었다. Spatial Pyramid Pooling networks(SPPnets)   앞서 R-CNN이 비용 낭비가 심하므로 R-CNN의 속도를 개선하기 위해 computation을 공유하도록 했다. 즉 SPPnets는 전체 이미지에서 convolutional feature map을 먼저 계산하고 feature map으로부터 pooling으로 얻은 feature vector를 분류하는 방법이다. 이러면 feature map이라는 계산을 공유하게 된다. 이 때 사용된 Spatial pyramid pooling은 feature map으로부터 다양한 사이즈로 영역을 분할하고 max-pooling을 통해 feature를 얻고 이들을 concatenation 시켜 fixed-length representation을 생성한다. 이를 FC로 넣어 training과 inference 속도를 개선하였다. training은 3배정도 빨라지고 수행 속도도 10배에서 100배까지 빨라졌다. R-CNN은 CNN이 도는 횟수가 많았으나 SPPnet은 CNN이 한 번만 수행되어 빨라질 수 있었으며 feature map에서 정보를 추출한다는 특징이다. 또한 중요한 건 pyramid layer로 부터 얻는 representation(feature vector)이므로 이것만 미리 정하면 FC의 input으로 넣을 수 있으며 최초의 input 크기에 제한을 받지 않게 된다. 기존의 AlexNet과 같은 신경망의 입력 size가 고정되는 것은 FC이기 때문이다. Drawback 하지만 SPPnets 역시 multi-stage pipeline 구조라는 한계를 지니며(feature 추출, network fine-tuning, SVM 학습, fitting b-box regressor) feature들을 모두 disk에 쓰여져야 한다. 그리고 R-CNN과 다르게 spatial pyramid pooling을 예측하는 convolutional layer는 update를 할 수 없다. (Q1) 그렇다면 이제 문제 해결 목표를 다시 정의 해 보자. R-CNN, SPPnet보다 정확한 object-detection multi-task loss를 이용한 single-stage training 학습 시 모든 network layer update가 가능해야 함. feature를 disk에 쓰는 과정이 없어야 함. 2) Fast R-CNN architecture and training   구조는 다음과 같다. ConvNet을 거쳐 max pooling layer을 통해 conv feature map을 얻는다. 이 때 이미지와 region-proposal을 한 번에 넣는다. regional-proposal을 얻기 위해 selectvie search를 사용한다. feature map으로 부터 fixed-length feature vector를 얻기 위해 각 object proposal의 RoI(Region of Interest) pooling layer을 거친다. 얻은 feature vector에 각각 FC와 연결된 Softmax classifier &amp;amp; b-box regressor 두 개의 브런치 레이어를 먹인다. Softmax classifier: object class와 배경 추정 b-box regressor: object class 좌표를 추정 R-CNN은 이미지를 warping했다면 Fast R-CNN은 feature map을 고정되게 warping해야 한다. 이를 위해 RoI pooling layer를 사용한다. 2-1) The RoI pooling layer   RoI layer는 위에서 설명한 SPPnets의 가장 단순한 케이스이다. SPPnet이 여러 size의 bin으로 영역을 나눴다면 RoI layer는 하나로 정해진 bin의 갯수로 RoI에 대해 한 번만 pooling을 진행한다. 이를 통해 fixed-length feature vector를 얻는다. 2-2) Fine-tuning for detection   Fast R-CNN의 Network의 weight들을 갱신하기 위해 back-propagation을 한다. 그림과 같이 모든 layer에 대해 학습이 가능한 1 stage pipeline으로 구성되어 error-backProp이 가능해 정확도 개선이 이루어졌다. 2-3) Region-wise sampling   Training time을 줄이기 위해 region-wise sampling 방법을 사용하였다. R-CNN의 경우 모든 region-proposal을 동일한 크기(224 × 224)로 바꾸지만 Fast R-CNN은 원본 그대로 사용하여 연산량이 증가하는 최악의 경우가 생긴다. 이를 피하기 위해 hierarchical sampling 방식을 사용한다. 기존의 R-CNN, SPPnet에서는 한 mini-batch 당 128개의 data에서 RoI를 무작위로 선택하였다. Fast R-CNN에서는 2개의 input으로부터 128개의 RoI(하나 당 64개)를 정함으로써 이미지의 RoI는 연산과 메모리를 공유하게 된다.(Q2) 3) Reference [1] paper : https://arxiv.org/abs/1504.08083 [2] https://tensorflow.blog/2017/06/05/from-r-cnn-to-mask-r-cnn/ [3] https://nittaku.tistory.com/273 [4] https://m.blog.naver.com/PostView.nhn?blogId=laonple&amp;amp;logNo=220776743537&amp;amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2019/08/23/Paperreview-Fast_R-CNN/">
  
  
  <link rel="alternate" type="application/rss+xml" title="pirunita" href="http://localhost:4000/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="[Paper review] Fast R-CNN">
  <meta name="twitter:description" content="Fast R-CNN 1) Introduction   전에 리뷰한 R-CNN에는 3가지에 문제가 있다. 3단계 모델을 거친 pipeline 구조 Region-proposal을 추출하기 위해 Selective search 알고리즘, feature vector을 얻기 위한 CNN, 그리고 feature vector로 분류하는 SVM classifier, 더...">
  
  
  
    





  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">pirunita</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/tagcloud/">TagCloud</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
    
      <h1 class="post-title" itemprop="name headline">[Paper review] Fast R-CNN</h1>
    
    <p class="post-meta"><time datetime="2019-08-23T15:02:00+00:00" itemprop="datePublished">Aug 23, 2019</time>
      <span>
        
          
          <a href="/tag/DeepLearning"><code class="highligher-rouge"><nobr>DeepLearning</nobr></code>&nbsp;</a>
        
          
          <a href="/tag/Paper"><code class="highligher-rouge"><nobr>Paper</nobr></code>&nbsp;</a>
        
      </span>
    </p>
  </header>
  
  <div class="post-content" itemprop="articleBody">
    <h2>Fast R-CNN</h2>
<hr />

<h2>1) Introduction</h2>
<p>  전에 리뷰한 R-CNN에는 3가지에 문제가 있다.</p>
<ol>
  <li>3단계 모델을 거친 pipeline 구조<br />
Region-proposal을 추출하기 위해 Selective search 알고리즘, feature vector을 얻기 위한 CNN, 그리고 feature vector로 분류하는 SVM classifier, 더 정확한 bounding-box를 얻기위한 regressor</li>
  <li>Training에 자원과 시간이 많이 소모<br />
각각 이미지에서 얻은 2000개의 region-proposal로부터 CNN을 통해 얻은 feature vector로 SVM과 bounding-box regressor를 훈련할 때 디스크에 이 data들을 모두 쓰면서 deep network를 training한다.</li>
  <li>느린 수행 속도<br />
역시나 각각의 test image에서 region proposal, feature vector을 얻는 데 시간이 걸린다.</li>
  <li>원치않는 이미지의 왜곡과 손실
CNN의 input을 맞추기 위해 warping 및 cropping이 일어나 정보의 손실 발생</li>
</ol>

<p>앞서 b-box 사이에 겹치는 영역을 따로따로 CNN에 통과시키는 것은 비용적으로 낭비이다. 그래서 제안된 것이 <strong>SPPnets</strong>이었다.</p>

<h3>Spatial Pyramid Pooling networks(SPPnets)</h3>
<p>  앞서 R-CNN이 비용 낭비가 심하므로 R-CNN의 속도를 개선하기 위해 computation을 공유하도록 했다. 즉 SPPnets는 전체 <em>이미지에서 convolutional feature map을 먼저 계산하고 feature map으로부터 pooling으로 얻은 feature vector를 분류하는 방법</em>이다. 이러면 feature map이라는 계산을 공유하게 된다.</p>

<p><img src="http://localhost:4000/assets/img/rcnn/fastrcnn1.png" alt="" width="70%" height="70%" class="center" /></p>

<p>이 때 사용된 <strong>Spatial pyramid pooling</strong>은 feature map으로부터 다양한 사이즈로 영역을 분할하고 max-pooling을 통해 feature를 얻고 이들을 concatenation 시켜 fixed-length representation을 생성한다. 이를 FC로 넣어 training과 inference 속도를 개선하였다. training은 3배정도 빨라지고 수행 속도도 10배에서 100배까지 빨라졌다.</p>

<p>R-CNN은 CNN이 도는 횟수가 많았으나 SPPnet은 CNN이 한 번만 수행되어 빨라질 수 있었으며 feature map에서 정보를 추출한다는 특징이다.</p>

<p>또한 중요한 건 pyramid layer로 부터 얻는 representation(feature vector)이므로 이것만 미리 정하면 FC의 input으로 넣을 수 있으며 최초의 input 크기에 제한을 받지 않게 된다. 기존의 AlexNet과 같은 신경망의 입력 size가 고정되는 것은 FC이기 때문이다.</p>

<h4>Drawback</h4>
<p>하지만 SPPnets 역시 multi-stage pipeline 구조라는 한계를 지니며(feature 추출, network fine-tuning, SVM 학습, fitting b-box regressor) feature들을 모두 disk에 쓰여져야 한다.</p>

<p>그리고 R-CNN과 다르게 spatial pyramid pooling을 예측하는 convolutional layer는 update를 할 수 없다. <strong>(Q1)</strong></p>

<p>그렇다면 이제 문제 해결 목표를 다시 정의 해 보자.</p>

<ol>
  <li>R-CNN, SPPnet보다 정확한 object-detection</li>
  <li>multi-task loss를 이용한 single-stage training</li>
  <li>학습 시 모든 network layer update가 가능해야 함.</li>
  <li>feature를 disk에 쓰는 과정이 없어야 함.</li>
</ol>

<h2>2) Fast R-CNN architecture and training</h2>
<p>  구조는 다음과 같다.</p>

<p><img src="http://localhost:4000/assets/img/rcnn/fastrcnn2.png" alt="" width="70%" height="70%" class="center" /></p>

<ol>
  <li>ConvNet을 거쳐 max pooling layer을 통해 conv feature map을 얻는다. 이 때 이미지와 region-proposal을 한 번에 넣는다. regional-proposal을 얻기 위해 selectvie search를 사용한다.</li>
  <li>feature map으로 부터 fixed-length feature vector를 얻기 위해 각 object proposal의 <strong>RoI(Region of Interest) pooling layer</strong>을 거친다.</li>
  <li>얻은 feature vector에 각각 FC와 연결된 Softmax classifier &amp; b-box regressor 두 개의 브런치 레이어를 먹인다.
    <ul>
      <li>Softmax classifier: object class와 배경 추정</li>
      <li>b-box regressor: object class 좌표를 추정</li>
    </ul>
  </li>
</ol>

<p>R-CNN은 이미지를 warping했다면 Fast R-CNN은 <em>feature map</em>을 고정되게 warping해야 한다. 이를 위해 <strong>RoI pooling layer</strong>를 사용한다.</p>

<h3>2-1) The RoI pooling layer</h3>
<p><img src="http://localhost:4000/assets/img/rcnn/fastrcnn3.png" alt="" width="70%" height="70%" class="center" /></p>

<p>  RoI layer는 위에서 설명한 SPPnets의 가장 단순한 케이스이다. SPPnet이 여러 size의 bin으로 영역을 나눴다면 RoI layer는 하나로 정해진 bin의 갯수로 RoI에 대해 한 번만 pooling을 진행한다. 이를 통해 fixed-length feature vector를 얻는다.</p>

<h3>2-2) Fine-tuning for detection</h3>
<p><img src="http://localhost:4000/assets/img/rcnn/fastrcnn4.png" alt="" width="70%" height="70%" class="center" /></p>

<p>  Fast R-CNN의 Network의 weight들을 갱신하기 위해 back-propagation을 한다. 그림과 같이 모든 layer에 대해 학습이 가능한 1 stage pipeline으로 구성되어 error-backProp이 가능해 정확도 개선이 이루어졌다.</p>

<h3>2-3) Region-wise sampling</h3>
<p>  Training time을 줄이기 위해 <strong>region-wise sampling</strong> 방법을 사용하였다. R-CNN의 경우 모든 region-proposal을 동일한 크기(224 × 224)로 바꾸지만 Fast R-CNN은 원본 그대로 사용하여 연산량이 증가하는 최악의 경우가 생긴다. 이를 피하기 위해 <strong>hierarchical sampling</strong> 방식을 사용한다.</p>

<p>기존의 R-CNN, SPPnet에서는 한 mini-batch 당 128개의 data에서 RoI를 무작위로 선택하였다. Fast R-CNN에서는 2개의 input으로부터 128개의 RoI(하나 당 64개)를 정함으로써 이미지의 RoI는 연산과 메모리를 공유하게 된다.<strong>(Q2)</strong></p>

<h2>3) Reference</h2>
<p>[1] paper : https://arxiv.org/abs/1504.08083</p>

<p>[2] https://tensorflow.blog/2017/06/05/from-r-cnn-to-mask-r-cnn/</p>

<p>[3] https://nittaku.tistory.com/273</p>

<p>[4] https://m.blog.naver.com/PostView.nhn?blogId=laonple&amp;logNo=220776743537&amp;proxyReferer=https%3A%2F%2Fwww.google.com%2F</p>

  </div>
  <div class="post-comments" itemprop="comment">
    <hr />
<h1>Comments</h1>
<p>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pirunita.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</p>

  </div>
  
  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">
    <p>
      <!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

    
  
    
  
    
    <li><a href="mailto:ksparchive39@gmail.com" class="icon-8 email" title="Email"><svg viewBox="0 0 512 512"><path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z"/></svg><!--[if lt IE 9]><em>Email</em><![endif]--></a></li>
    
  
    
  
    
    
    
    <li><a href="https://github.com/pirunita" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
    
  
    
  
    
    <li><a href="https://instagram.com/pirunita.dev" class="icon-15 instagram" title="Instagram"><svg viewBox="0 0 512 512"><g><path d="M256 109.3c47.8 0 53.4 0.2 72.3 1 17.4 0.8 26.9 3.7 33.2 6.2 8.4 3.2 14.3 7.1 20.6 13.4 6.3 6.3 10.1 12.2 13.4 20.6 2.5 6.3 5.4 15.8 6.2 33.2 0.9 18.9 1 24.5 1 72.3s-0.2 53.4-1 72.3c-0.8 17.4-3.7 26.9-6.2 33.2 -3.2 8.4-7.1 14.3-13.4 20.6 -6.3 6.3-12.2 10.1-20.6 13.4 -6.3 2.5-15.8 5.4-33.2 6.2 -18.9 0.9-24.5 1-72.3 1s-53.4-0.2-72.3-1c-17.4-0.8-26.9-3.7-33.2-6.2 -8.4-3.2-14.3-7.1-20.6-13.4 -6.3-6.3-10.1-12.2-13.4-20.6 -2.5-6.3-5.4-15.8-6.2-33.2 -0.9-18.9-1-24.5-1-72.3s0.2-53.4 1-72.3c0.8-17.4 3.7-26.9 6.2-33.2 3.2-8.4 7.1-14.3 13.4-20.6 6.3-6.3 12.2-10.1 20.6-13.4 6.3-2.5 15.8-5.4 33.2-6.2C202.6 109.5 208.2 109.3 256 109.3M256 77.1c-48.6 0-54.7 0.2-73.8 1.1 -19 0.9-32.1 3.9-43.4 8.3 -11.8 4.6-21.7 10.7-31.7 20.6 -9.9 9.9-16.1 19.9-20.6 31.7 -4.4 11.4-7.4 24.4-8.3 43.4 -0.9 19.1-1.1 25.2-1.1 73.8 0 48.6 0.2 54.7 1.1 73.8 0.9 19 3.9 32.1 8.3 43.4 4.6 11.8 10.7 21.7 20.6 31.7 9.9 9.9 19.9 16.1 31.7 20.6 11.4 4.4 24.4 7.4 43.4 8.3 19.1 0.9 25.2 1.1 73.8 1.1s54.7-0.2 73.8-1.1c19-0.9 32.1-3.9 43.4-8.3 11.8-4.6 21.7-10.7 31.7-20.6 9.9-9.9 16.1-19.9 20.6-31.7 4.4-11.4 7.4-24.4 8.3-43.4 0.9-19.1 1.1-25.2 1.1-73.8s-0.2-54.7-1.1-73.8c-0.9-19-3.9-32.1-8.3-43.4 -4.6-11.8-10.7-21.7-20.6-31.7 -9.9-9.9-19.9-16.1-31.7-20.6 -11.4-4.4-24.4-7.4-43.4-8.3C310.7 77.3 304.6 77.1 256 77.1L256 77.1z"/><path d="M256 164.1c-50.7 0-91.9 41.1-91.9 91.9s41.1 91.9 91.9 91.9 91.9-41.1 91.9-91.9S306.7 164.1 256 164.1zM256 315.6c-32.9 0-59.6-26.7-59.6-59.6s26.7-59.6 59.6-59.6 59.6 26.7 59.6 59.6S288.9 315.6 256 315.6z"/><circle cx="351.5" cy="160.5" r="21.5"/></g></svg><!--[if lt IE 9]><em>Instagram</em><![endif]--></a></li>
    
  
    
    <li><a href="https://www.linkedin.com/in/pirunita" class="icon-17 linkedin" title="LinkedIn"><svg viewBox="0 0 512 512"><path d="M186.4 142.4c0 19-15.3 34.5-34.2 34.5 -18.9 0-34.2-15.4-34.2-34.5 0-19 15.3-34.5 34.2-34.5C171.1 107.9 186.4 123.4 186.4 142.4zM181.4 201.3h-57.8V388.1h57.8V201.3zM273.8 201.3h-55.4V388.1h55.4c0 0 0-69.3 0-98 0-26.3 12.1-41.9 35.2-41.9 21.3 0 31.5 15 31.5 41.9 0 26.9 0 98 0 98h57.5c0 0 0-68.2 0-118.3 0-50-28.3-74.2-68-74.2 -39.6 0-56.3 30.9-56.3 30.9v-25.2H273.8z"/></svg><!--[if lt IE 9]><em>LinkedIn</em><![endif]--></a></li>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
  </ul>
    </p>
    
    <p>
      

&copy; pirunita - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
