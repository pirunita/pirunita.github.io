<!DOCTYPE html>
<html lang="ko">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>StarGAN</title>
  <meta name="description" content="StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Choi, Yunjey, et al. “Stargan: Unified generative adversarial networks for multi-domain image-to-image translation.” arXiv preprint 1711 (2017). 1. Introduction   image-to-image translation은 한 이미지에서 다른 이미지로 바꾸는 기술이다. 특히 Generative Adversarial Networks(GANs)로 image-to-image translation의 기술을 크게 발전시켰다. 두 개의 서로 다른 domain으로부터 training data가 주어졌을 때 model은 한 domain에서 다른 domain으로 image translation을 하도록 학습한다. 이 때의 term들을 다음과 같이 정의한다. attribute : meaningful feature(hair color, gender, age..) attribute value : a particular value of an attribute(brown, black, male, female..) domain : a set of images sharing the same attribute 특히 몇몇 image datasets은 수많은 labeled attributes와 관련이 있다. CelebA dataset : facial attributes(머리색, 성별, 나이 등..)와 관련된 40개의 label들을 가지고 있다. RaFD dataset : facial expressions(happy, angry, sad)와 같은 8개의 label들을 가지고 있다. StarGAN은 이러한 점에서 multiple domain으로부터 attribute에 따라 image를 바꾸는 multi-domain image-to-image translation을 생각해낸 것이다. Figure 1의 우측은 RaFD를 학습하면서 얻은 feature를 이용하여 CelebA와 RaFD jointly training으로 CelebA image의 표정을 바꾸는 학습을 진행한다.   그러나 기존의 모델은 multi-domain image translation에 매우 비효율적이다. 그 이유는 다음 3가지와 같다. k개의 domain 간 mapping을 학습할 때 (k-1)개의 generator를 학습해야 하기 때문이다. 또한 각각의 generator들은 전체 training data를 완전히 사용하지 못하고 k개의 domain 중에 2개만 학습한다. 이는 생성되는 이미지의 품질 저하를 일으킨다. 게다가 기존의 모델에서는 서로 다른 dataset으로부터 jointly training domain이 불가능하다. 왜냐하면 각각의 dataset은 partially labeled이기 때문에..Section 3.2   StarGAN은 위의 그림처럼 하나의 generator로 여러 multiple domain 사이의 mapping을 학습시키는 모델 구조를 제안한다. 또한 input으로는 image와 domain information을 넣는다. 이 때 domain information은 label(binary나 one-hot vector)을 사용한다. 그리고 서로 다른 dataset의 domain의 joint training을 위해 domain label에 mask vector라는 정보를 추가한다. 이 방법은 알려지지 않는 label은 무시하고 특정 dataset의 label에만 집중 할 수 있게 된다. 따라서 요악하면 다음과 같다. 각각의 domain의 image로부터 효율적으로 image-to-image translation을 학습하기 위해 single generator와 discriminator을 이용하여 multiple domains의 mapping을 학습하는 starGAN 제안 domain labels를 control하기 위해 mask vector를 사용하여 multi domain image translation 학습 StarGAN을 이용하여 facial attribute transfer와 facial expression synthesis에서 좋은 성과를 얻음 2. Related Work Generative Adversarial Networks 생성되는 이미지가 더욱 실감나도록 adversarial loss를 leverage Conditional GANs 본 논문에서는 conditional domain information을 넣기 위해 scalabe GAN framework를 사용한다. 이는 다양한 domain에서 image translation을 유연하게 control할 수 있도록 한다. Image-to-Image Translation CycleGAN과 DiscoGAN은 cycle consistency loss를 활용하여 input과 translated image 사이의 key attributes를 보존하므로 즉, 복원된 이미지가 입력 이미지와 비슷하게 학습시키기 위해 starGAN에서도 cycle consistency loss를 추가한다. 3. Star Generative Adversarial Networks 이번 섹션에서는 서로 다른 label을 갖는 multiple dataset을 가지고 StarGAN으로 label을 통해 image translation 시키는 지 알아본다. 3.1. Multi-Domain Image-to-Image Translation   먼저 Generator를 학습시키기 위해서 input: x, target domain label: c, output: y G(x, c) → y 또한 auxiliary classifier을 통해 하나의 discriminator가 multiple domain을 control할 수 있도록 하였다. Discrimintor은 source와 domain label의 probability distributions을 생성한다. D : x → {Dsrc(x), Dcls(x)} Adversarial Loss   생성된 이미지와 원본 이미지를 구별하기 위해 adversarial loss를 사용한다. 여기서 Dsrc(x)는 discriminator D에 의해 생성된 source의 probability distribution을 뜻한다. Generator G는 최소화되고 discriminator D는 최대화하도록 할 것이다. Domain Classification Loss   input x와 target domain label c가 주어졌을 때 우리는 x에서 target domain c로 적절히 분리 된 output y를 만드는 것이다. 이를 위해서 D의 상단에 auxiliary classifier을 추가하고 D와 G를 최적화 할 때 domain classification loss를 추가한다. 정리하면 다음과 같다. 실제 이미지의 domain classification loss → D를 최적화 가짜 이미지의 domain classification loss → G를 최적화 여기서 Dcls(c’ㅣx)는 D에 의해 계산된 domain label의 probability distribution이다. 따라서 D는 결국 이 classification loss를 최소화시켜, original domain c’에 해당하는 real image를 분류할 수 있다. 반면에 fake image의 domain classification에 대한 loss funcion은 다음과 같다. 여기서 G는 위 식의 loss를 줄여나가며 target domain c에 분류되는 이미지를 생성하도록 한다. Reconstruction Loss   위의 설명했던 adversarial loss와 classification losses를 최소화함으로써 G는 더 현실적인 이미지를 생성하도록 학습하고 target domain에 정확히 대응되도록 분류한다. “However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.” 하지만 loss들을 최소화하면 바뀐 이미지가 input과 관련된 domain을 바꾸는 동안 입력 이미지의 내용을 유지한다는 보장이 없다.(?) 따라서 generator에 cycle consistency loss를 적용시켜 문제를 해결하고자 했다. 이는 다음과 같다. 여기서 G는 변환이미지 G(x,c)와 original domain label c’를 input으로 다시 original image x를 재구축(reconstruction)하게 되며 L1 norm을 reconstruction loss로 사용하였다. 이 때 주목할 점은 single generator을 두 번 사용한다 그렇다면 이제 다음 그림을 보면서 StarGAN의 전반적인 학습 프로세스를 요악한다. G(x:input image, c:target domain) → y:Fake image G(y:Fake image, c’:original domain) → x:reconstructed image D : Distinguish between real &amp;amp; fake + auxiliary classifier(domain label classification) Full Objective   G와 D를 최적화하는 objective functions는 다음과 같다. 3.2. Training with Multiple Datasets   StarGAN의 중요한 장점은 전혀 다른 label을 포함한 multiple dataset을 동시에 적용시켜 모든 label을 control할 수 있다는 점이다. 앞서 설명드린 서로 다른 label을 갖는 CelebA와 RaFD의 경우다. 예를 들어 CelebA는 attribute를 갖지만 facial expression label은 갖고 있지 않다. 이를 해결하기 위해서 reconstruction process의 label vector c’를 추가할 것이다.. Mask Vector StarGAN은 mask vector m을 통해서 필요없는 label은 무시하고 특정 dataset의 label에 집중하도록 도와준다. mask vector m을 나타내기 위해서 datasets의 수(data의 수가 아님) n개가 있다면 n-dimensional one-hot vector을 구성한다. Ci는 i번째 dataset의 label을 나타내는 vector이다. label ci를 표현하는 방법은 binary attribute의 binary vector 혹은 categorical attribute을 나타내는 one-hot vector일 수 있다. 이 때 나머지 n-1개의 unknown labels는 모두 0으로 준다. Training Strategy   위에 설명드렸던 domain label vector c ̃를 generator의 input으로 넣을 것이다. 그러면 generator은 불필요한 label(zero vector)을 무시하고 주어진 label을 더욱 집중할 수 있을 것이다.. “By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label. “ G의 구조는 input label c ̃의 차원이 아닌 single dataset 학습 할 때와 동일하다. 모든 dataset에 대한 label에 대해 probability distribution을 만들기 위해 D의 auxiliary classfier를 확장한다. D가 인식 된 label과 관련된 classification error만을 최소화시키기 위해 multi-task에서 학습을 수행한다. 4. Implementation Improved GAN Training   학습을 안정화시키고 high quality image를 만들기 위해 Adversarial loss를 gradient penalty가 포함 된 Wasserstein GAN objective로 바꾼다. ↓ λgp = 10으로 설정한다. x^는 한 pair의 real image와 생성된 이미지 사이의 직선을 따라 균일하게 샘플링 된다.(?) Network Architecture   StarGaN의 generator network와 discriminator는 다음과 같이 구성되어 있다. Generator network architecture Discriminator network architecture 5. Experiments 5.1. Baseline Models DIAT CycleGAN IcGAN 5.2. Dataset CelebA : 202,599 face images of celebrities, 40 binary attributes, 7 domains Attributes : hair color(black, blond, brown), gender(male/female), age(young/old) RaFD : 4,824 images collected from 67 participants, 8 facial expression in 3 different gaze directions 5.3. Training Using Adam optimizer, β1 = 0.5, β2 = 0.999 Batch size : 16 Learning rate CelebA : 0.0001에서 10 epochs마다 감소 RaFD : 0.0001에서 100 epochs 마다 감소 Source : single NVIDIA Tesla M40 GPU 5.4. Experimental Results on CelebA   먼저 CelebA로만 학습시켰을 때의 결과다. multiple attribute에서 합성을 진행하였으며, 좋은 퀄리티를 낼 수 있었다. 5.5. Experimental Results on RaFD   다음은 RaFD로만 학습을 시켰을 때의 결과이다. 5.6. Experimental Results on CelebA + RaFD   Multi Domatin뿐만 아니라 Multi dataset으로도 학습시킨 결과이다. StarGAN(SNG) : RaFD로 학습시킨 모델로 CelebA에 적용시킨 결과 StarGAN(JNT) : RaFD + CelebA로 학습시킨 모델로 CelebA에 적용시킨 결과 multi dataset으로 학습시킨 모델이 조금 더 사진을 잘 생성해내는 것을 알 수 있다.">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2018/12/08/StarGAN/">
  
  
  <link rel="alternate" type="application/rss+xml" title="pirunita" href="http://localhost:4000/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="StarGAN">
  <meta name="twitter:description" content="StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Choi, Yunjey, et al. “Stargan: Unified generative adversarial networks for multi-domain image-to-image t...">
  
  
  
    





  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">pirunita</a>

    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/tagcloud/">TagCloud</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
        
        <a class="page-link" href="/contact/">Contact</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
    
      <h1 class="post-title" itemprop="name headline">StarGAN</h1>
    
    <p class="post-meta"><time datetime="2018-12-08T08:17:00+00:00" itemprop="datePublished">Dec 8, 2018</time>
      <span>
        
          
          <a href="/tag/DeepLearning"><code class="highligher-rouge"><nobr>DeepLearning</nobr></code>&nbsp;</a>
        
          
          <a href="/tag/GAN"><code class="highligher-rouge"><nobr>GAN</nobr></code>&nbsp;</a>
        
      </span>
    </p>
  </header>
  
  <div class="post-content" itemprop="articleBody">
    <h1>StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation</h1>

<h5>Choi, Yunjey, et al. “Stargan: Unified generative adversarial networks for multi-domain image-to-image translation.” arXiv preprint 1711 (2017).</h5>
<hr />

<h2>1. Introduction</h2>
<p>  image-to-image translation은 한 이미지에서 다른 이미지로 바꾸는 기술이다. 특히 Generative Adversarial Networks(GANs)로 image-to-image translation의 기술을 크게 발전시켰다.
<br />
<br />
두 개의 서로 다른 <strong>domain</strong>으로부터 training data가 주어졌을 때 model은 한 domain에서 다른 domain으로 image translation을 하도록 학습한다. 이 때의 term들을 다음과 같이 정의한다.</p>
<ul>
  <li>attribute : meaningful feature(hair color, gender, age..)</li>
  <li>attribute value : a particular value of an attribute(brown, black, male, female..)</li>
  <li>domain : a set of images sharing the same attribute
<br /></li>
</ul>

<p><img src="http://localhost:4000/assets/img/StarGAN/01.png" alt="" /></p>

<p>특히 몇몇 image datasets은 수많은 labeled attributes와 관련이 있다.</p>
<ul>
  <li>CelebA dataset : facial attributes(머리색, 성별, 나이 등..)와 관련된 40개의 label들을 가지고 있다.</li>
  <li>RaFD dataset : facial expressions(happy, angry, sad)와 같은 8개의 label들을 가지고 있다.</li>
</ul>

<p>StarGAN은 이러한 점에서 multiple domain으로부터 attribute에 따라 image를 바꾸는 multi-domain image-to-image translation을 생각해낸 것이다. Figure 1의 우측은 RaFD를 학습하면서 얻은 feature를 이용하여 CelebA와 RaFD jointly training으로 CelebA image의 표정을 바꾸는 학습을 진행한다.</p>

<p><img src="http://localhost:4000/assets/img/StarGAN/02.png" alt="" width="50%" height="50%" class="center" /></p>

<p>  그러나 기존의 모델은 multi-domain image translation에 매우 비효율적이다. 그 이유는 다음 3가지와 같다.</p>
<ul>
  <li>k개의 domain 간 mapping을 학습할 때 (k-1)개의 generator를 학습해야 하기 때문이다.</li>
  <li>또한 각각의 generator들은 전체 training data를 완전히 사용하지 못하고 k개의 domain 중에 2개만 학습한다. 이는 생성되는 이미지의 <strong>품질 저하</strong>를 일으킨다.</li>
  <li>게다가 기존의 모델에서는 서로 다른 dataset으로부터 jointly training domain이 불가능하다. 왜냐하면 각각의 dataset은 <em>partially labeled</em>이기 때문에..<a href="#sec3_2">Section 3.2</a>
<br />
<br />
<img src="http://localhost:4000/assets/img/StarGAN/03.png" alt="" width="50%" height="50%" class="center" /></li>
</ul>

<p>  StarGAN은 위의 그림처럼 하나의 generator로 여러 multiple domain 사이의 mapping을 학습시키는 모델 구조를 제안한다.
<br />
또한 input으로는 <strong>image</strong>와 <strong>domain information</strong>을 넣는다. 이 때 domain information은 label(binary나 one-hot vector)을 사용한다.<br /><br />
그리고 서로 다른 dataset의 domain의 joint training을 위해 domain label에 <strong>mask vector</strong>라는 정보를 추가한다. 이 방법은 알려지지 않는 label은 <strong>무시하고</strong> 특정 dataset의 label에만 <strong>집중</strong> 할 수 있게 된다.</p>

<p>따라서 요악하면 다음과 같다.</p>
<ul>
  <li>각각의 domain의 image로부터 효율적으로 image-to-image translation을 학습하기 위해 <strong>single generator와 discriminator</strong>을 이용하여 multiple domains의 mapping을 학습하는 starGAN 제안</li>
  <li>domain labels를 control하기 위해 <strong>mask vector</strong>를 사용하여 multi domain image translation 학습</li>
  <li>StarGAN을 이용하여 facial attribute transfer와 facial expression synthesis에서 좋은 성과를 얻음</li>
</ul>

<p><br /><br /></p>
<h2>2. Related Work</h2>
<h4><b>Generative Adversarial Networks</b></h4>
<p>생성되는 이미지가 더욱 실감나도록 adversarial loss를 leverage</p>

<h4><b>Conditional GANs</b></h4>
<p>본 논문에서는 conditional domain information을 넣기 위해 <strong>scalabe GAN framework</strong>를 사용한다. 이는 다양한 domain에서 image translation을 유연하게 control할 수 있도록 한다.</p>

<h4><b>Image-to-Image Translation</b></h4>
<p>CycleGAN과 DiscoGAN은 <strong>cycle consistency loss</strong>를 활용하여 input과 translated image 사이의 key attributes를 보존하므로 즉, 복원된 이미지가 입력 이미지와 비슷하게 학습시키기 위해 starGAN에서도 cycle consistency loss를 추가한다.</p>

<p><br /><br /></p>
<h2>3. Star Generative Adversarial Networks</h2>
<p>이번 섹션에서는 서로 다른 label을 갖는 multiple dataset을 가지고 StarGAN으로 label을 통해 image translation 시키는 지 알아본다.
<img src="http://localhost:4000/assets/img/StarGAN/04.png" alt="" class="center" /></p>

<h3>3.1. Multi-Domain Image-to-Image Translation</h3>
<p>  먼저 Generator를 학습시키기 위해서<br />
input: x, target domain label: c, output: y<br />
G(x, c) → y<br /><br />
또한 auxiliary classifier을 통해 하나의 discriminator가 multiple domain을 control할 수 있도록 하였다. Discrimintor은 source와 domain label의 <strong>probability distributions</strong>을 생성한다.<br />
D : x → {D<sub>src</sub>(x), D<sub>cls</sub>(x)}</p>

<h4><b>Adversarial Loss</b></h4>
<p>  생성된 이미지와 원본 이미지를 구별하기 위해 adversarial loss를 사용한다.
<img src="http://localhost:4000/assets/img/StarGAN/05.png" alt="" width="60%" height="60%" class="center" /></p>

<p>여기서 D<sub>src</sub>(x)는 discriminator D에 의해 생성된 source의 probability distribution을 뜻한다.<br />
Generator G는 최소화되고 discriminator D는 최대화하도록 할 것이다.</p>

<h4><b>Domain Classification Loss</b></h4>
<p>  input x와 target domain label c가 주어졌을 때 우리는 x에서 target domain c로 적절히 분리 된 output y를 만드는 것이다. 이를 위해서 D의 상단에 <strong>auxiliary classifier</strong>을 추가하고 D와 G를 최적화 할 때 <strong>domain classification loss</strong>를 추가한다. 정리하면 다음과 같다. <br /></p>
<ul>
  <li>실제 이미지의 domain classification loss → D를 최적화</li>
  <li>가짜 이미지의 domain classification loss → G를 최적화</li>
</ul>

<p><img src="http://localhost:4000/assets/img/StarGAN/06.png" alt="" width="60%" height="60%" class="center" /></p>

<p>여기서 D<sub>cls</sub>(c’ㅣx)는 D에 의해 계산된 domain label의 probability distribution이다. 따라서 D는 결국 이 classification loss를 최소화시켜, original domain c’에 해당하는 real image를 분류할 수 있다.<br /><br /></p>

<p>반면에 fake image의 domain classification에 대한 loss funcion은 다음과 같다.
<img src="http://localhost:4000/assets/img/StarGAN/07.png" alt="" width="60%" height="60%" class="center" />
여기서 G는 위 식의 loss를 줄여나가며 target domain c에 분류되는 이미지를 생성하도록 한다.</p>

<h4><b>Reconstruction Loss</b></h4>
<p>  위의 설명했던 adversarial loss와 classification losses를 최소화함으로써 G는 더 현실적인 이미지를 생성하도록 학습하고 target domain에 정확히 대응되도록 분류한다. <br />
“However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.”<br />
하지만 loss들을 최소화하면 바뀐 이미지가 input과 관련된 domain을 바꾸는 동안 입력 이미지의 내용을 유지한다는 보장이 없다.(?)<br />
따라서 generator에 <strong>cycle consistency loss</strong>를 적용시켜 문제를 해결하고자 했다. 이는 다음과 같다.
<img src="http://localhost:4000/assets/img/StarGAN/08.png" alt="" width="60%" height="60%" class="center" /></p>

<p>여기서 G는 변환이미지 G(x,c)와 original domain label c’를 input으로 다시 original image x를 <strong>재구축(reconstruction)</strong>하게 되며 L1 norm을 reconstruction loss로 사용하였다. 이 때 주목할 점은 <em>single generator을 두 번 사용한다</em>
<br /><br />
그렇다면 이제 다음 그림을 보면서 StarGAN의 전반적인 학습 프로세스를 요악한다. 
<br />
<img src="http://localhost:4000/assets/img/StarGAN/14.png" alt="" class="center" />
<img src="http://localhost:4000/assets/img/StarGAN/15.png" alt="" class="center" /></p>

<ul>
  <li>G(x:input image, c:target domain) → y:Fake image</li>
  <li>G(y:Fake image, c’:original domain) → x:reconstructed image</li>
  <li>D : Distinguish between real &amp; fake + auxiliary classifier(domain label classification)</li>
</ul>

<p><br /></p>
<h4><b>Full Objective</b></h4>
<p>  G와 D를 최적화하는 objective functions는 다음과 같다.
<img src="http://localhost:4000/assets/img/StarGAN/09.png" alt="" width="60%" height="60%" class="center" /></p>

<p id="sec3_2">
</p>
<p><br /></p>

<h3>3.2. Training with Multiple Datasets</h3>

<p>  StarGAN의 중요한 장점은 전혀 다른 label을 포함한 multiple dataset을 동시에 적용시켜 모든 label을 control할 수 있다는 점이다. 앞서 설명드린 서로 다른 label을 갖는 CelebA와 RaFD의 경우다. 예를 들어 CelebA는 attribute를 갖지만 facial expression label은 갖고 있지 않다. 이를 해결하기 위해서 reconstruction process의 <b>label vector c’</b>를 추가할 것이다..
<br /></p>
<h4><b>Mask Vector</b></h4>
<p>StarGAN은 mask vector m을 통해서 필요없는 label은 무시하고 특정 dataset의 label에 집중하도록 도와준다. mask vector m을 나타내기 위해서 datasets의 수(data의 수가 아님) n개가 있다면 <strong>n-dimensional one-hot vector</strong>을 구성한다. 
<img src="http://localhost:4000/assets/img/StarGAN/10.png" alt="" width="40%" height="40%" class="center" /><br />
C<sub>i</sub>는 i번째 dataset의 label을 나타내는 vector이다. label c<sub>i</sub>를 표현하는 방법은 binary attribute의 <strong>binary vector</strong> 혹은 categorical attribute을 나타내는 <strong>one-hot vector</strong>일 수 있다. 이 때 나머지 n-1개의 unknown labels는 모두 0으로 준다.</p>

<h4><b>Training Strategy</b></h4>
<p>  위에 설명드렸던 domain label vector c ̃를 generator의 input으로 넣을 것이다. 그러면 generator은 불필요한 label(zero vector)을 무시하고 주어진 label을 더욱 집중할 수 있을 것이다..<br />
<b>
“By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label.
“
</b></p>
<ul>
  <li>G의 구조는 input label c ̃의 차원이 아닌 single dataset 학습 할 때와 동일하다.</li>
  <li>모든 dataset에 대한 label에 대해 probability distribution을 만들기 위해 D의 auxiliary classfier를 확장한다.</li>
  <li>D가 인식 된 label과 관련된 classification error만을 최소화시키기 위해 multi-task에서 학습을 수행한다.
<br /><br /></li>
</ul>

<h2>4. Implementation</h2>
<h4><b>Improved GAN Training</b></h4>
<p>  학습을 안정화시키고 high quality image를 만들기 위해 Adversarial loss를 gradient penalty가 포함 된 <strong>Wasserstein GAN objective</strong>로 바꾼다.
<img src="http://localhost:4000/assets/img/StarGAN/05.png" alt="" width="60%" height="60%" class="center" /></p>
<center>↓</center>
<p><br />
<img src="http://localhost:4000/assets/img/StarGAN/11.png" alt="" width="60%" height="60%" class="center" /><br />
λ<sub>gp</sub> = 10으로 설정한다. x^는 한 pair의 real image와 생성된 이미지 사이의 직선을 따라 균일하게 샘플링 된다.(?)<br /></p>

<h4><b>Network Architecture</b></h4>
<p>  StarGaN의 generator network와 discriminator는 다음과 같이 구성되어 있다.</p>
<ul>
  <li>Generator network architecture
<img src="http://localhost:4000/assets/img/StarGAN/12.png" alt="" width="80%" height="80%" class="center" /><br /></li>
  <li>Discriminator network architecture
<img src="http://localhost:4000/assets/img/StarGAN/13.png" alt="" width="80%" height="80%" class="center" />
<br /><br /></li>
</ul>

<h2>5. Experiments</h2>

<h3>5.1. Baseline Models</h3>
<ul>
  <li>DIAT</li>
  <li>CycleGAN</li>
  <li>IcGAN</li>
</ul>

<h3>5.2. Dataset</h3>
<ul>
  <li>CelebA : 202,599 face images of celebrities, 40 binary attributes, 7 domains
    <ul>
      <li>Attributes : hair color(black, blond, brown), gender(male/female), age(young/old)</li>
    </ul>
  </li>
  <li>RaFD : 4,824 images collected from 67 participants, 8 facial expression in 3 different gaze directions</li>
</ul>

<h3>5.3. Training</h3>
<ul>
  <li>Using Adam optimizer, β<sub>1</sub> = 0.5, β<sub>2</sub> = 0.999</li>
  <li>Batch size : 16</li>
  <li>Learning rate
    <ul>
      <li>CelebA : 0.0001에서 10 epochs마다 감소</li>
      <li>RaFD : 0.0001에서 100 epochs 마다 감소</li>
    </ul>
  </li>
  <li>Source : single NVIDIA Tesla M40 GPU</li>
</ul>

<h3>5.4. Experimental Results on CelebA</h3>

<p><img src="http://localhost:4000/assets/img/StarGAN/16.png" alt="" class="center" />
  먼저 CelebA로만 학습시켰을 때의 결과다. multiple attribute에서 합성을 진행하였으며, 좋은 퀄리티를 낼 수 있었다.</p>

<h3>5.5. Experimental Results on RaFD</h3>
<p><img src="http://localhost:4000/assets/img/StarGAN/17.png" alt="" class="center" />
  다음은 RaFD로만 학습을 시켰을 때의 결과이다.</p>

<h3>5.6. Experimental Results on CelebA + RaFD</h3>
<p><img src="http://localhost:4000/assets/img/StarGAN/18.png" alt="" class="center" />
  Multi Domatin뿐만 아니라 Multi dataset으로도 학습시킨 결과이다.</p>
<ul>
  <li>StarGAN(SNG) : RaFD로 학습시킨 모델로 CelebA에 적용시킨 결과</li>
  <li>StarGAN(JNT) : RaFD + CelebA로 학습시킨 모델로 CelebA에 적용시킨 결과</li>
</ul>

<p>multi dataset으로 학습시킨 모델이 조금 더 사진을 잘 생성해내는 것을 알 수 있다.</p>


  </div>
  <div class="post-comments" itemprop="comment">
    <hr />
<h1>Comments</h1>
<p>Insert your custom comment provider like 
  <a href="https://disqus.com">Disqus</a> or <a href="https://posativ.org/isso">Isso
  
  </a>
  <div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pirunita.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</p>

  </div>
  
  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <p>
      

&copy; pirunita - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
