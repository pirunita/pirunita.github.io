<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <generator uri="http://jekyllrb.com" version="3.8.5">Jekyll</generator>
  
  
  <link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en" />
  <updated>2019-03-31T00:20:34+00:00</updated>
  <id>http://localhost:4000//</id>

  
    <title type="html">pirunita</title>
  

  
    <subtitle>pirunita</subtitle>
  

  
    <author>
        <name>pirunita</name>
      
      
    </author>
  

  
  
    <entry xml:lang="ko">
      
      <title type="html">딥러닝 공부 기초</title>
      
      
      <link href="http://localhost:4000/2018/12/23/DeepLearningBasic/" rel="alternate" type="text/html" title="딥러닝 공부 기초" />
      
      <published>2018-12-23T14:00:00+00:00</published>
      <updated>2018-12-23T14:00:00+00:00</updated>
      <id>http://localhost:4000/2018/12/23/DeepLearningBasic</id>
      <content type="html" xml:base="http://localhost:4000/2018/12/23/DeepLearningBasic/">&lt;h4&gt;Deep Learning from Scratch(밑바닥부터 시작하는 딥러닝), Saito koki, 한빛미디어&lt;/h4&gt;
&lt;hr /&gt;

&lt;p&gt;Chapter 1, 2 : 생략&lt;br /&gt;
Chapter 3 : &lt;a href=&quot;https://github.com/pirunita/DeepLearningBasic/blob/master/Chapter%203.%20Neural%20Network.ipynb&quot;&gt;[click]&lt;/a&gt;&lt;br /&gt;
Chapter 4 : &lt;a href=&quot;https://github.com/pirunita/DeepLearningBasic/blob/master/Chapter%204.%20Neural%20Network%20Training.ipynb&quot;&gt;[click]&lt;/a&gt;&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>pirunita</name>
          
          
        </author>
      

      

      
        <category term="DeepLearning" />
      

      
        <summary type="html">Deep Learning from Scratch(밑바닥부터 시작하는 딥러닝), Saito koki, 한빛미디어 Chapter 1, 2 : 생략 Chapter 3 : [click] Chapter 4 : [click]</summary>
      

      
      
    </entry>
  
  
  
    <entry xml:lang="ko">
      
      <title type="html">M2E-Try On Net</title>
      
      
      <link href="http://localhost:4000/2018/12/14/M2E-Try-On-Net/" rel="alternate" type="text/html" title="M2E-Try On Net" />
      
      <published>2018-12-14T12:47:00+00:00</published>
      <updated>2018-12-14T12:47:00+00:00</updated>
      <id>http://localhost:4000/2018/12/14/M2E-Try%20On%20Net</id>
      <content type="html" xml:base="http://localhost:4000/2018/12/14/M2E-Try-On-Net/">&lt;h1&gt;M2E-Try On Net: Fashion from Model to Everyone&lt;/h1&gt;

&lt;h5&gt;Zhonghua Wu, et al. “M2E-Try On Net: Fashion from Model to Everyone” arXiv: 1811.08599 (2018).&lt;/h5&gt;
&lt;hr /&gt;

&lt;h2&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;  VITON처럼 3D 정보 없이 virtual-try-on system을 만들고 싶다. M2E에서 하고자 하는 것은 요약하면 다음 3가지와 같다.
&lt;br /&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/M2E-TON/01.png&quot; alt=&quot;&quot; width=&quot;70%&quot; height=&quot;70%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;M2E-TON이라 불리는 virtual Try-On Network는 원하는 모델의 옷을 임의의 사람 사진에 자동으로 바꿔준다. 이는 M2E-TON가 디자인 한 sub-network들을 통해 texture을 보존하면서 사람의 pose에 따라 잘 정립된 real한 이미지를 만들어 줄 것이다. 특히 M2E-TON은 &lt;u&gt;product image가 필요 없다는 것&lt;/u&gt;이 큰 특징이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;M2E-TON에서는 3가지 sub-network를 제안한다.
    &lt;ul&gt;
      &lt;li&gt;model image에서 target person pose로 정렬되는 &lt;strong&gt;pose alignment network(PAN)&lt;/strong&gt; 도입&lt;/li&gt;
      &lt;li&gt;옷의 특징을 더욱 끌어내기 위해 pose가 정렬 된 이미지에 변형된 clothes textures를 추가하는 &lt;strong&gt;Texture Refinement Network(TRN)&lt;/strong&gt;을 도입&lt;/li&gt;
      &lt;li&gt;원하는 옷을 target person image에 맞추기 위한 &lt;strong&gt;Fitting Network(FTN)&lt;/strong&gt;을 도입&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;target person P와 원하는 model clothes인 P’는 unpair한 학습 데이터이기 때문에 &lt;strong&gt;unsupervised learning &amp;amp; self-supervised learning&lt;/strong&gt;의 &lt;u&gt;hybrid learning framework&lt;/u&gt;을 사용한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2&gt;2. Related Works&lt;/h2&gt;
&lt;p&gt;  M2E-TON은 human parsing &amp;amp; analysis, person image generation, virtul try-on, fashion dataset과 관련이 있다.&lt;/p&gt;

&lt;h3&gt;2.1. Human Parsing and Human Pose Estimation&lt;/h3&gt;
&lt;p&gt;  더 정확한 pose estimation을 위해 &lt;strong&gt;DensePose&lt;/strong&gt;를 이용한다. DensePose는 각 픽셀마다 dense pose point를 mapping시켜 human pose estimation을 얻는 방법이다. M2E-TON에서는 clothes region warping과 pose alignment을 위해 &lt;strong&gt;estimated dense poses&lt;/strong&gt;를 활용한다.
&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;2.2. Person Image Generation and Virtual Try-On&lt;/h3&gt;
&lt;p&gt;  이전에 시도되었던 virtual try on은 product image를 input으로 넣는 방식이었다.&lt;br /&gt;
하지만 M2E-TON은 identity와 pose를 유지하면서 model pearson에서 target person으로 자연스럽게 변할 수 있다.&lt;/p&gt;

&lt;h3&gt;2.3. Fashion Datasets&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Deep Fashion dataset : clothes attribute prediction과 landmark detection을 위한 fashion dataset&lt;/li&gt;
  &lt;li&gt;MVC dataset : invariant clothing retrieval and attribute prediction&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2&gt;3. Approach&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/M2E-TON/02.png&quot; alt=&quot;&quot; width=&quot;70%&quot; height=&quot;70%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;3.1. Pose Alignment Network (PAN)&lt;/h3&gt;

&lt;h3&gt;3.2. Texture Refinement Network (TRN)&lt;/h3&gt;

&lt;h3&gt;3.3. Fitting Network (FTN)&lt;/h3&gt;

&lt;h3&gt;3.4. Unpair-Pair Joint Training&lt;/h3&gt;

&lt;h3&gt;3.5. Loss Functions&lt;/h3&gt;</content>

      
      
      
      
      

      
        <author>
            <name>pirunita</name>
          
          
        </author>
      

      

      
        <category term="DeepLearning" />
      
        <category term="GAN" />
      
        <category term="Virtual-Try-On" />
      

      
        <summary type="html">M2E-Try On Net: Fashion from Model to Everyone Zhonghua Wu, et al. “M2E-Try On Net: Fashion from Model to Everyone” arXiv: 1811.08599 (2018). 1. Introduction   VITON처럼 3D 정보 없이 virtual-try-on system을 만들고 싶다. M2E에서 하고자 하는 것은 요약하면 다음 3가지와 같다. M2E-TON이라 불리는 virtual Try-On Network는 원하는 모델의 옷을 임의의 사람 사진에 자동으로 바꿔준다. 이는 M2E-TON가 디자인 한 sub-network들을 통해 texture을 보존하면서 사람의 pose에 따라 잘 정립된 real한 이미지를 만들어 줄 것이다. 특히 M2E-TON은 product image가 필요 없다는 것이 큰 특징이다. M2E-TON에서는 3가지 sub-network를 제안한다. model image에서 target person pose로 정렬되는 pose alignment network(PAN) 도입 옷의 특징을 더욱 끌어내기 위해 pose가 정렬 된 이미지에 변형된 clothes textures를 추가하는 Texture Refinement Network(TRN)을 도입 원하는 옷을 target person image에 맞추기 위한 Fitting Network(FTN)을 도입 target person P와 원하는 model clothes인 P’는 unpair한 학습 데이터이기 때문에 unsupervised learning &amp;amp; self-supervised learning의 hybrid learning framework을 사용한다. 2. Related Works   M2E-TON은 human parsing &amp;amp; analysis, person image generation, virtul try-on, fashion dataset과 관련이 있다. 2.1. Human Parsing and Human Pose Estimation   더 정확한 pose estimation을 위해 DensePose를 이용한다. DensePose는 각 픽셀마다 dense pose point를 mapping시켜 human pose estimation을 얻는 방법이다. M2E-TON에서는 clothes region warping과 pose alignment을 위해 estimated dense poses를 활용한다. 2.2. Person Image Generation and Virtual Try-On   이전에 시도되었던 virtual try on은 product image를 input으로 넣는 방식이었다. 하지만 M2E-TON은 identity와 pose를 유지하면서 model pearson에서 target person으로 자연스럽게 변할 수 있다. 2.3. Fashion Datasets Deep Fashion dataset : clothes attribute prediction과 landmark detection을 위한 fashion dataset MVC dataset : invariant clothing retrieval and attribute prediction 3. Approach 3.1. Pose Alignment Network (PAN) 3.2. Texture Refinement Network (TRN) 3.3. Fitting Network (FTN) 3.4. Unpair-Pair Joint Training 3.5. Loss Functions</summary>
      

      
      
    </entry>
  
  
  
    <entry xml:lang="ko">
      
      <title type="html">VITON</title>
      
      
      <link href="http://localhost:4000/2018/12/13/VITON/" rel="alternate" type="text/html" title="VITON" />
      
      <published>2018-12-13T16:13:00+00:00</published>
      <updated>2018-12-13T16:13:00+00:00</updated>
      <id>http://localhost:4000/2018/12/13/VITON</id>
      <content type="html" xml:base="http://localhost:4000/2018/12/13/VITON/">&lt;h1&gt;VITON: An Image-based Virtual Try-on Network&lt;/h1&gt;

&lt;h5&gt;Xintong Han, et al. “VITON: An Image-based Virtual Try-on Network” arXiv: 1711.08447 (2017).&lt;/h5&gt;
&lt;hr /&gt;

&lt;h2&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;  VITON은 어떠한 3D 정보 없이 plain RGB image만을 갖고 image기반의 virtual try-on을 제안한다. VITON은 옷을 착용한 사람의 영역에 균일하게 상품이미지를 overlaying시켜 photo-realistic한 이미지를 합성하고자 한다. 이 때 합성되는 이미지에는 몇 가지 issue들이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;원본 이미지에서 body parts와 사람의 pose가 같아야 한다.&lt;/li&gt;
  &lt;li&gt;상품 이미지의 옷이 사람의 pose와 body shape에 맞춰 자연스럽게 변형되어야 한다.&lt;/li&gt;
  &lt;li&gt;상품의 low-level feature(color, texture)나 복잡한 그래픽(로고, 자수)와 같은 디테일한 시각적 패턴이 명확하게 드러나야 한다.
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  Conditional Generative Adversarial Networks는 다양한 이미지 처리에서 좋은 결과를 보여준 네트워크이기 때문에 이 문제를 해결하기 위해 자연스럽게 사용할 것이다. 특히 CGAN은 &lt;strong&gt;adversarial loss&lt;/strong&gt;를 최소화하여 input signal을 조건으로 generator에서 생성된 이미지가 discriminator에 의해 진짜 이미지와 구별될 수 없도록 한다. &lt;u&gt;하지만 CGAN은 object단위의 클래스나 attribute를 rough하게 변형하기 때문에 디테일하거나 기하학적인 변화를 기대하기는 어렵다는 한계&lt;/u&gt;가 있다.&lt;br /&gt;
  이러한 한계를 해결하기 위해 VITON에서는 2D image 기반으로 target clothing item에서 옷을 입은 사람의 영역으로 균일하게 바꾸는 새로운 coarse-to-fine framework를 제안한다. 특히, 사람의 서로 다른 특징을 표현하기 위해서 &lt;strong&gt;clothing-agnostic representation&lt;/strong&gt;을 도입한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Multi-task encoder-decoder network : target clothing item을 옷을 입히고 싶은 마스크(clothing region mask)에 맞춰 같은 포즈로 &lt;strong&gt;coarse synthetic clothed person&lt;/strong&gt;을 생성한다. 이 때 &lt;strong&gt;clothing region mask&lt;/strong&gt;는 옷을 변형하는 데 가이드라인으로 사용 될 것이다.&lt;/li&gt;
  &lt;li&gt;refinement network : 이 네트워크는 warping된 옷이 coarse image에 얼마나 잘 합성되는 지 학습하며, 이를 통해 원하는 옷이 자연스럽게 변형되어 입혀질 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;2. Related Work&lt;/h2&gt;
&lt;h4&gt;&lt;b&gt; Fashion analysis &lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;2D image들을 input으로 넣어 vitual try-on에 중점을 두었다.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt; Image synthesis &lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;당연히 알고 있는 것이지만 &lt;strong&gt;conditional GANs&lt;/strong&gt;을 통해 image-to-image translation을 수행한다. 최근에는 adversarial training을 사용하지 않고 &lt;strong&gt;regression loss&lt;/strong&gt;를 사용한 CNN을 통해 더욱 photo-realistic한 이미지를 생성할 수 있었다.(CRN, Cascaded Refinement Networks) 하지만 기하학적인 변형에는 한계가 있다. (CycleGAN) 대신에 VITON은 &lt;strong&gt;refinement network&lt;/strong&gt;를 통해 clothing region에 집중하고 clothing deformation을 처리할 것이다.
&lt;br /&gt;&lt;br /&gt;
(1. 최근에 CRN과 CycleGAN을 비교해 좋은 성능을 보여준 논문을 봤는데 pix2pixHD였나..다시 봐야 할 듯..)&lt;br /&gt;
(2. CRN은 기하학적인 변형이 불가능하다면 CRN과 refinement network의 차이는 무엇일까)&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Fashion application에서 image synthesis은 다양하게 나왔다. 여기서 제시 된 가장 관련된 것은 &lt;strong&gt;Fashion GAN&lt;/strong&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h4&gt;&lt;b&gt; Virtual try-on &lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;다양한 virtual try-on들이 소개 된다. 가장 주목할만 한 것은 &lt;strong&gt;conditional analogy GAN&lt;/strong&gt;으로 swap fashion을 하는 데, 이 작업의 문제는 target item과 original item을 각각 입고 있어야 하는 데이터가 필요하므로 실용적이지 못하며 person representation이 전혀 들어가지 않아 현실적이지 못함.
&lt;br /&gt;&lt;/p&gt;

&lt;h2&gt;&lt;u&gt;3. VITON&lt;/u&gt;&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/VITON/01.png&quot; alt=&quot;&quot; width=&quot;70%&quot; height=&quot;70%&quot; class=&quot;center&quot; /&gt;
&lt;br /&gt;
  옷을 입고있는 사람 reference image I와 그 target clothing item c를 통해 새로운 image I&lt;sup&gt;^&lt;/sup&gt;를 합성하는 것이다. 이 때 target clothing c는 그 사람의 body parts와 pose 값들을 통해 해당 영역에 맞춰 변형될 것이다. 핵심은 &lt;u&gt;product image가 몸에 맞게 적절히 변형되도록 학습하는 것&lt;/u&gt;이다.&lt;br /&gt;
  보다 실용적인 가상 피팅 시나리오를 위해서 test할 때는 reference image, 즉 &lt;u&gt;입히고 싶은 사람&lt;/u&gt;사진과 원하는 상품이미지만 필요하다. 따라서 보다 실용적인 학습을 위해서 학습 진행 시 input으로 상품 이미지 c와 c를 입고 있는 reference image I가 들어가게 된다. 하지만 이것만으로는 generator가 사람의 정보를 통해 옷을 변형시킬 수 없기 때문에 다음과 같은 과정을 진행으로 학습한다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Clothing-agnostic person representation&lt;/li&gt;
  &lt;li&gt;Encoder-decoder architecture&lt;/li&gt;
  &lt;li&gt;Refinement network&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;3.1. Person Representation&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/VITON/02.png&quot; alt=&quot;&quot; width=&quot;80%&quot; height=&quot;80%&quot; class=&quot;center&quot; /&gt;
&lt;br /&gt;
  VITON에서 보다 기술적인 도전은 사람의 자세에 맞춰 의류 이미지를 변형시키는 것이다. 따라서 VITON은 위 그림과 같이 &lt;u&gt;pose, body parts, Face and hair&lt;/u&gt;의 정보를 포함하는 &lt;b&gt;clothing-agnostic person representation&lt;/b&gt;를 도입하였다.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Pose heatmap&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  사람의 자세에 따라서 옷은 다양하게 변하게 된다. 따라서 VITON에서는 최신 pose estimator를 사용하여 사람의 pose를 &lt;u&gt;18개의 keypoint를 좌표&lt;/u&gt;로 나타냈다. 그리고 공간 레이아웃의 활용을 위해 각 키 포인트는 &lt;u&gt;그 주변이 11 * 11이 1로 채워지는 &lt;b&gt;heatmap&lt;/b&gt;&lt;/u&gt;으로 구성되고 나머지는 0으로 채워진다.&lt;br /&gt;&lt;br /&gt;
  → keypoint로 부터 18 채널의 &lt;strong&gt;pose heatmap&lt;/strong&gt;으로 구성&lt;br /&gt;&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Human body representation&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  옷의 형태는 신체의 모양에 크게 의존하기 때문에 target clothing을 변형시키는 것은 &lt;strong&gt;신체 부위&lt;/strong&gt;와 &lt;strong&gt;신체 형태&lt;/strong&gt;에 따라 달라진다. 따라서 &lt;strong&gt;human parser&lt;/strong&gt;를 통해 신체를 영역별로 추출하는 &lt;strong&gt;human segmentation map&lt;/strong&gt;을 얻는다.
  또한 VITON에서는 segmentation map을 &lt;strong&gt;1-channel binary mask&lt;/strong&gt;(1 : 얼굴과 머리를 제외한 신체, 0 : 그 외)를 얻어 16 * 12의 저해상도 이미지를 위 그림과 같이 얻는다. 이를 통해 body shape와 target clothing간의 충돌을 방지할 수 있다.(왜?)
&lt;br /&gt;&lt;br /&gt;
  → segmentation map으로 부터 1 채널의 &lt;strong&gt;binary mask&lt;/strong&gt;로 구성&lt;br /&gt;&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Face and hair segment&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  사진 속 인물의 특성을 살리기 위해서는 얼굴, 피부 색, 머리 모양과 같은 &lt;strong&gt;physical attributes&lt;/strong&gt;를 추출해야 한다. 따라서 VITON에서는 &lt;strong&gt;human parser&lt;/strong&gt;를 사용하여 이미지를 생성할 때 &lt;strong&gt;얼굴의 RGB channel&lt;/strong&gt;과 &lt;strong&gt;hair region&lt;/strong&gt;을 추출한다.
&lt;br /&gt;&lt;br /&gt;
  → human parser로 부터 3 채널의 &lt;strong&gt;face color &amp;amp; hair region&lt;/strong&gt;으로 구성&lt;br /&gt;&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Concatenation&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/VITON/03.png&quot; alt=&quot;&quot; width=&quot;20%&quot; height=&quot;20%&quot; class=&quot;center&quot; /&gt;&lt;br /&gt;
  최종적으로 clothing-agnostic person representation p를 형성하기 위해서 위의 3가지 feature map을 같은 해상도로 resize 및 concatenate를 진행한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;m : 256, height of the feature map&lt;/li&gt;
  &lt;li&gt;n : 192, widht of the feature map&lt;/li&gt;
  &lt;li&gt;k : 18(pose heat map) + 1(segment to binary mask) + 3(human parser RGB) = 22의 채널 수를 나타낸다.
&lt;br /&gt;
    &lt;h3&gt;3.2. Multi-task Encoder-Decoder Generator&lt;/h3&gt;
    &lt;p&gt;  clothing-agnostic person representation p와 target clothing image c를 통해 reference image I를 &lt;strong&gt;reconstruction&lt;/strong&gt;하여 p영역에 c를 다시 입힐 수 있도록 학습한다. 이 때 쓰이는 &lt;strong&gt;Multi-task Encoder-Decoder framework&lt;/strong&gt;를 통해 clothing mask를 따라 옷을 입은 사람 이미지를 형성할 것이다. 추가적으로, clothing region에 초점을 둔 network를 따라서 예측된 clothing mask는 refinement network에 활용된다. Encoder-decoder은 U-net 구조의 일반적 형태이며 &lt;strong&gt;skip connection&lt;/strong&gt;을 통해 레이어 간 정보를 직접 공유한다.&lt;br /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/VITON/01.png&quot; alt=&quot;&quot; width=&quot;70%&quot; height=&quot;70%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;  G&lt;sub&gt;c&lt;/sub&gt;를 encoder-decoder generator라고 정의하고 concatenate된 c와 p를 input으로 넣고 4-channel의 output을 만든다.&lt;br /&gt;
(I’, M) = G&lt;sub&gt;c&lt;/sub&gt;(c, p)에서 3-channels은 합성 된 이미지 I’를 나타내고 last channel M은 위의 그림에서처럼 clothing region의 segmentation mask를 나타낸다. VITON은 generator가 I’를 I와 가깝게, M을 M&lt;sub&gt;0&lt;/sub&gt;(M&lt;sub&gt;0&lt;/sub&gt;는 I의 human parser에 의해 예측된 &lt;strong&gt;pseudo ground truth clothing mask&lt;/strong&gt;)와 가깝게 학습한다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;3.3. Refinement Network&lt;/h3&gt;

&lt;h4&gt;&lt;b&gt;Warped clothing item&lt;/b&gt;&lt;/h4&gt;

&lt;h4&gt;&lt;b&gt;Learn to composite&lt;/b&gt;&lt;/h4&gt;

&lt;h3&gt;Detailed Network Structures&lt;/h3&gt;</content>

      
      
      
      
      

      
        <author>
            <name>pirunita</name>
          
          
        </author>
      

      

      
        <category term="DeepLearning" />
      
        <category term="GAN" />
      
        <category term="Virtual-Try-On" />
      

      
        <summary type="html">VITON: An Image-based Virtual Try-on Network Xintong Han, et al. “VITON: An Image-based Virtual Try-on Network” arXiv: 1711.08447 (2017). 1. Introduction   VITON은 어떠한 3D 정보 없이 plain RGB image만을 갖고 image기반의 virtual try-on을 제안한다. VITON은 옷을 착용한 사람의 영역에 균일하게 상품이미지를 overlaying시켜 photo-realistic한 이미지를 합성하고자 한다. 이 때 합성되는 이미지에는 몇 가지 issue들이 있다. 원본 이미지에서 body parts와 사람의 pose가 같아야 한다. 상품 이미지의 옷이 사람의 pose와 body shape에 맞춰 자연스럽게 변형되어야 한다. 상품의 low-level feature(color, texture)나 복잡한 그래픽(로고, 자수)와 같은 디테일한 시각적 패턴이 명확하게 드러나야 한다.   Conditional Generative Adversarial Networks는 다양한 이미지 처리에서 좋은 결과를 보여준 네트워크이기 때문에 이 문제를 해결하기 위해 자연스럽게 사용할 것이다. 특히 CGAN은 adversarial loss를 최소화하여 input signal을 조건으로 generator에서 생성된 이미지가 discriminator에 의해 진짜 이미지와 구별될 수 없도록 한다. 하지만 CGAN은 object단위의 클래스나 attribute를 rough하게 변형하기 때문에 디테일하거나 기하학적인 변화를 기대하기는 어렵다는 한계가 있다.   이러한 한계를 해결하기 위해 VITON에서는 2D image 기반으로 target clothing item에서 옷을 입은 사람의 영역으로 균일하게 바꾸는 새로운 coarse-to-fine framework를 제안한다. 특히, 사람의 서로 다른 특징을 표현하기 위해서 clothing-agnostic representation을 도입한다. Multi-task encoder-decoder network : target clothing item을 옷을 입히고 싶은 마스크(clothing region mask)에 맞춰 같은 포즈로 coarse synthetic clothed person을 생성한다. 이 때 clothing region mask는 옷을 변형하는 데 가이드라인으로 사용 될 것이다. refinement network : 이 네트워크는 warping된 옷이 coarse image에 얼마나 잘 합성되는 지 학습하며, 이를 통해 원하는 옷이 자연스럽게 변형되어 입혀질 것이다. 2. Related Work Fashion analysis 2D image들을 input으로 넣어 vitual try-on에 중점을 두었다. Image synthesis 당연히 알고 있는 것이지만 conditional GANs을 통해 image-to-image translation을 수행한다. 최근에는 adversarial training을 사용하지 않고 regression loss를 사용한 CNN을 통해 더욱 photo-realistic한 이미지를 생성할 수 있었다.(CRN, Cascaded Refinement Networks) 하지만 기하학적인 변형에는 한계가 있다. (CycleGAN) 대신에 VITON은 refinement network를 통해 clothing region에 집중하고 clothing deformation을 처리할 것이다. (1. 최근에 CRN과 CycleGAN을 비교해 좋은 성능을 보여준 논문을 봤는데 pix2pixHD였나..다시 봐야 할 듯..) (2. CRN은 기하학적인 변형이 불가능하다면 CRN과 refinement network의 차이는 무엇일까) Fashion application에서 image synthesis은 다양하게 나왔다. 여기서 제시 된 가장 관련된 것은 Fashion GAN Virtual try-on 다양한 virtual try-on들이 소개 된다. 가장 주목할만 한 것은 conditional analogy GAN으로 swap fashion을 하는 데, 이 작업의 문제는 target item과 original item을 각각 입고 있어야 하는 데이터가 필요하므로 실용적이지 못하며 person representation이 전혀 들어가지 않아 현실적이지 못함. 3. VITON   옷을 입고있는 사람 reference image I와 그 target clothing item c를 통해 새로운 image I^를 합성하는 것이다. 이 때 target clothing c는 그 사람의 body parts와 pose 값들을 통해 해당 영역에 맞춰 변형될 것이다. 핵심은 product image가 몸에 맞게 적절히 변형되도록 학습하는 것이다.   보다 실용적인 가상 피팅 시나리오를 위해서 test할 때는 reference image, 즉 입히고 싶은 사람사진과 원하는 상품이미지만 필요하다. 따라서 보다 실용적인 학습을 위해서 학습 진행 시 input으로 상품 이미지 c와 c를 입고 있는 reference image I가 들어가게 된다. 하지만 이것만으로는 generator가 사람의 정보를 통해 옷을 변형시킬 수 없기 때문에 다음과 같은 과정을 진행으로 학습한다. Clothing-agnostic person representation Encoder-decoder architecture Refinement network 3.1. Person Representation   VITON에서 보다 기술적인 도전은 사람의 자세에 맞춰 의류 이미지를 변형시키는 것이다. 따라서 VITON은 위 그림과 같이 pose, body parts, Face and hair의 정보를 포함하는 clothing-agnostic person representation를 도입하였다. Pose heatmap   사람의 자세에 따라서 옷은 다양하게 변하게 된다. 따라서 VITON에서는 최신 pose estimator를 사용하여 사람의 pose를 18개의 keypoint를 좌표로 나타냈다. 그리고 공간 레이아웃의 활용을 위해 각 키 포인트는 그 주변이 11 * 11이 1로 채워지는 heatmap으로 구성되고 나머지는 0으로 채워진다.   → keypoint로 부터 18 채널의 pose heatmap으로 구성 Human body representation   옷의 형태는 신체의 모양에 크게 의존하기 때문에 target clothing을 변형시키는 것은 신체 부위와 신체 형태에 따라 달라진다. 따라서 human parser를 통해 신체를 영역별로 추출하는 human segmentation map을 얻는다.   또한 VITON에서는 segmentation map을 1-channel binary mask(1 : 얼굴과 머리를 제외한 신체, 0 : 그 외)를 얻어 16 * 12의 저해상도 이미지를 위 그림과 같이 얻는다. 이를 통해 body shape와 target clothing간의 충돌을 방지할 수 있다.(왜?)   → segmentation map으로 부터 1 채널의 binary mask로 구성 Face and hair segment   사진 속 인물의 특성을 살리기 위해서는 얼굴, 피부 색, 머리 모양과 같은 physical attributes를 추출해야 한다. 따라서 VITON에서는 human parser를 사용하여 이미지를 생성할 때 얼굴의 RGB channel과 hair region을 추출한다.   → human parser로 부터 3 채널의 face color &amp;amp; hair region으로 구성 Concatenation   최종적으로 clothing-agnostic person representation p를 형성하기 위해서 위의 3가지 feature map을 같은 해상도로 resize 및 concatenate를 진행한다. m : 256, height of the feature map n : 192, widht of the feature map k : 18(pose heat map) + 1(segment to binary mask) + 3(human parser RGB) = 22의 채널 수를 나타낸다. 3.2. Multi-task Encoder-Decoder Generator   clothing-agnostic person representation p와 target clothing image c를 통해 reference image I를 reconstruction하여 p영역에 c를 다시 입힐 수 있도록 학습한다. 이 때 쓰이는 Multi-task Encoder-Decoder framework를 통해 clothing mask를 따라 옷을 입은 사람 이미지를 형성할 것이다. 추가적으로, clothing region에 초점을 둔 network를 따라서 예측된 clothing mask는 refinement network에 활용된다. Encoder-decoder은 U-net 구조의 일반적 형태이며 skip connection을 통해 레이어 간 정보를 직접 공유한다.   Gc를 encoder-decoder generator라고 정의하고 concatenate된 c와 p를 input으로 넣고 4-channel의 output을 만든다. (I’, M) = Gc(c, p)에서 3-channels은 합성 된 이미지 I’를 나타내고 last channel M은 위의 그림에서처럼 clothing region의 segmentation mask를 나타낸다. VITON은 generator가 I’를 I와 가깝게, M을 M0(M0는 I의 human parser에 의해 예측된 pseudo ground truth clothing mask)와 가깝게 학습한다. 3.3. Refinement Network Warped clothing item Learn to composite Detailed Network Structures</summary>
      

      
      
    </entry>
  
  
  
    <entry xml:lang="ko">
      
      <title type="html">StarGAN</title>
      
      
      <link href="http://localhost:4000/2018/12/08/StarGAN/" rel="alternate" type="text/html" title="StarGAN" />
      
      <published>2018-12-08T08:17:00+00:00</published>
      <updated>2018-12-08T08:17:00+00:00</updated>
      <id>http://localhost:4000/2018/12/08/StarGAN</id>
      <content type="html" xml:base="http://localhost:4000/2018/12/08/StarGAN/">&lt;h1&gt;StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation&lt;/h1&gt;

&lt;h5&gt;Choi, Yunjey, et al. “Stargan: Unified generative adversarial networks for multi-domain image-to-image translation.” arXiv preprint 1711 (2017).&lt;/h5&gt;
&lt;hr /&gt;

&lt;h2&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;  image-to-image translation은 한 이미지에서 다른 이미지로 바꾸는 기술이다. 특히 Generative Adversarial Networks(GANs)로 image-to-image translation의 기술을 크게 발전시켰다.
&lt;br /&gt;
&lt;br /&gt;
두 개의 서로 다른 &lt;strong&gt;domain&lt;/strong&gt;으로부터 training data가 주어졌을 때 model은 한 domain에서 다른 domain으로 image translation을 하도록 학습한다. 이 때의 term들을 다음과 같이 정의한다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;attribute : meaningful feature(hair color, gender, age..)&lt;/li&gt;
  &lt;li&gt;attribute value : a particular value of an attribute(brown, black, male, female..)&lt;/li&gt;
  &lt;li&gt;domain : a set of images sharing the same attribute
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/01.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;특히 몇몇 image datasets은 수많은 labeled attributes와 관련이 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;CelebA dataset : facial attributes(머리색, 성별, 나이 등..)와 관련된 40개의 label들을 가지고 있다.&lt;/li&gt;
  &lt;li&gt;RaFD dataset : facial expressions(happy, angry, sad)와 같은 8개의 label들을 가지고 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;StarGAN은 이러한 점에서 multiple domain으로부터 attribute에 따라 image를 바꾸는 multi-domain image-to-image translation을 생각해낸 것이다. Figure 1의 우측은 RaFD를 학습하면서 얻은 feature를 이용하여 CelebA와 RaFD jointly training으로 CelebA image의 표정을 바꾸는 학습을 진행한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/02.png&quot; alt=&quot;&quot; width=&quot;50%&quot; height=&quot;50%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;  그러나 기존의 모델은 multi-domain image translation에 매우 비효율적이다. 그 이유는 다음 3가지와 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;k개의 domain 간 mapping을 학습할 때 (k-1)개의 generator를 학습해야 하기 때문이다.&lt;/li&gt;
  &lt;li&gt;또한 각각의 generator들은 전체 training data를 완전히 사용하지 못하고 k개의 domain 중에 2개만 학습한다. 이는 생성되는 이미지의 &lt;strong&gt;품질 저하&lt;/strong&gt;를 일으킨다.&lt;/li&gt;
  &lt;li&gt;게다가 기존의 모델에서는 서로 다른 dataset으로부터 jointly training domain이 불가능하다. 왜냐하면 각각의 dataset은 &lt;em&gt;partially labeled&lt;/em&gt;이기 때문에..&lt;a href=&quot;#sec3_2&quot;&gt;Section 3.2&lt;/a&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/03.png&quot; alt=&quot;&quot; width=&quot;50%&quot; height=&quot;50%&quot; class=&quot;center&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;  StarGAN은 위의 그림처럼 하나의 generator로 여러 multiple domain 사이의 mapping을 학습시키는 모델 구조를 제안한다.
&lt;br /&gt;
또한 input으로는 &lt;strong&gt;image&lt;/strong&gt;와 &lt;strong&gt;domain information&lt;/strong&gt;을 넣는다. 이 때 domain information은 label(binary나 one-hot vector)을 사용한다.&lt;br /&gt;&lt;br /&gt;
그리고 서로 다른 dataset의 domain의 joint training을 위해 domain label에 &lt;strong&gt;mask vector&lt;/strong&gt;라는 정보를 추가한다. 이 방법은 알려지지 않는 label은 &lt;strong&gt;무시하고&lt;/strong&gt; 특정 dataset의 label에만 &lt;strong&gt;집중&lt;/strong&gt; 할 수 있게 된다.&lt;/p&gt;

&lt;p&gt;따라서 요악하면 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;각각의 domain의 image로부터 효율적으로 image-to-image translation을 학습하기 위해 &lt;strong&gt;single generator와 discriminator&lt;/strong&gt;을 이용하여 multiple domains의 mapping을 학습하는 starGAN 제안&lt;/li&gt;
  &lt;li&gt;domain labels를 control하기 위해 &lt;strong&gt;mask vector&lt;/strong&gt;를 사용하여 multi domain image translation 학습&lt;/li&gt;
  &lt;li&gt;StarGAN을 이용하여 facial attribute transfer와 facial expression synthesis에서 좋은 성과를 얻음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;2. Related Work&lt;/h2&gt;
&lt;h4&gt;&lt;b&gt;Generative Adversarial Networks&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;생성되는 이미지가 더욱 실감나도록 adversarial loss를 leverage&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Conditional GANs&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;본 논문에서는 conditional domain information을 넣기 위해 &lt;strong&gt;scalabe GAN framework&lt;/strong&gt;를 사용한다. 이는 다양한 domain에서 image translation을 유연하게 control할 수 있도록 한다.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Image-to-Image Translation&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;CycleGAN과 DiscoGAN은 &lt;strong&gt;cycle consistency loss&lt;/strong&gt;를 활용하여 input과 translated image 사이의 key attributes를 보존하므로 즉, 복원된 이미지가 입력 이미지와 비슷하게 학습시키기 위해 starGAN에서도 cycle consistency loss를 추가한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2&gt;3. Star Generative Adversarial Networks&lt;/h2&gt;
&lt;p&gt;이번 섹션에서는 서로 다른 label을 갖는 multiple dataset을 가지고 StarGAN으로 label을 통해 image translation 시키는 지 알아본다.
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/04.png&quot; alt=&quot;&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;3.1. Multi-Domain Image-to-Image Translation&lt;/h3&gt;
&lt;p&gt;  먼저 Generator를 학습시키기 위해서&lt;br /&gt;
input: x, target domain label: c, output: y&lt;br /&gt;
G(x, c) → y&lt;br /&gt;&lt;br /&gt;
또한 auxiliary classifier을 통해 하나의 discriminator가 multiple domain을 control할 수 있도록 하였다. Discrimintor은 source와 domain label의 &lt;strong&gt;probability distributions&lt;/strong&gt;을 생성한다.&lt;br /&gt;
D : x → {D&lt;sub&gt;src&lt;/sub&gt;(x), D&lt;sub&gt;cls&lt;/sub&gt;(x)}&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Adversarial Loss&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  생성된 이미지와 원본 이미지를 구별하기 위해 adversarial loss를 사용한다.
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/05.png&quot; alt=&quot;&quot; width=&quot;60%&quot; height=&quot;60%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 D&lt;sub&gt;src&lt;/sub&gt;(x)는 discriminator D에 의해 생성된 source의 probability distribution을 뜻한다.&lt;br /&gt;
Generator G는 최소화되고 discriminator D는 최대화하도록 할 것이다.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Domain Classification Loss&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  input x와 target domain label c가 주어졌을 때 우리는 x에서 target domain c로 적절히 분리 된 output y를 만드는 것이다. 이를 위해서 D의 상단에 &lt;strong&gt;auxiliary classifier&lt;/strong&gt;을 추가하고 D와 G를 최적화 할 때 &lt;strong&gt;domain classification loss&lt;/strong&gt;를 추가한다. 정리하면 다음과 같다. &lt;br /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;실제 이미지의 domain classification loss → D를 최적화&lt;/li&gt;
  &lt;li&gt;가짜 이미지의 domain classification loss → G를 최적화&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/06.png&quot; alt=&quot;&quot; width=&quot;60%&quot; height=&quot;60%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 D&lt;sub&gt;cls&lt;/sub&gt;(c’ㅣx)는 D에 의해 계산된 domain label의 probability distribution이다. 따라서 D는 결국 이 classification loss를 최소화시켜, original domain c’에 해당하는 real image를 분류할 수 있다.&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;반면에 fake image의 domain classification에 대한 loss funcion은 다음과 같다.
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/07.png&quot; alt=&quot;&quot; width=&quot;60%&quot; height=&quot;60%&quot; class=&quot;center&quot; /&gt;
여기서 G는 위 식의 loss를 줄여나가며 target domain c에 분류되는 이미지를 생성하도록 한다.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Reconstruction Loss&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  위의 설명했던 adversarial loss와 classification losses를 최소화함으로써 G는 더 현실적인 이미지를 생성하도록 학습하고 target domain에 정확히 대응되도록 분류한다. &lt;br /&gt;
“However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.”&lt;br /&gt;
하지만 loss들을 최소화하면 바뀐 이미지가 input과 관련된 domain을 바꾸는 동안 입력 이미지의 내용을 유지한다는 보장이 없다.(?)&lt;br /&gt;
따라서 generator에 &lt;strong&gt;cycle consistency loss&lt;/strong&gt;를 적용시켜 문제를 해결하고자 했다. 이는 다음과 같다.
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/08.png&quot; alt=&quot;&quot; width=&quot;60%&quot; height=&quot;60%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 G는 변환이미지 G(x,c)와 original domain label c’를 input으로 다시 original image x를 &lt;strong&gt;재구축(reconstruction)&lt;/strong&gt;하게 되며 L1 norm을 reconstruction loss로 사용하였다. 이 때 주목할 점은 &lt;em&gt;single generator을 두 번 사용한다&lt;/em&gt;
&lt;br /&gt;&lt;br /&gt;
그렇다면 이제 다음 그림을 보면서 StarGAN의 전반적인 학습 프로세스를 요악한다. 
&lt;br /&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/14.png&quot; alt=&quot;&quot; class=&quot;center&quot; /&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/15.png&quot; alt=&quot;&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;G(x:input image, c:target domain) → y:Fake image&lt;/li&gt;
  &lt;li&gt;G(y:Fake image, c’:original domain) → x:reconstructed image&lt;/li&gt;
  &lt;li&gt;D : Distinguish between real &amp;amp; fake + auxiliary classifier(domain label classification)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h4&gt;&lt;b&gt;Full Objective&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  G와 D를 최적화하는 objective functions는 다음과 같다.
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/09.png&quot; alt=&quot;&quot; width=&quot;60%&quot; height=&quot;60%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p id=&quot;sec3_2&quot;&gt;
&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;3.2. Training with Multiple Datasets&lt;/h3&gt;

&lt;p&gt;  StarGAN의 중요한 장점은 전혀 다른 label을 포함한 multiple dataset을 동시에 적용시켜 모든 label을 control할 수 있다는 점이다. 앞서 설명드린 서로 다른 label을 갖는 CelebA와 RaFD의 경우다. 예를 들어 CelebA는 attribute를 갖지만 facial expression label은 갖고 있지 않다. 이를 해결하기 위해서 reconstruction process의 &lt;b&gt;label vector c’&lt;/b&gt;를 추가할 것이다..
&lt;br /&gt;&lt;/p&gt;
&lt;h4&gt;&lt;b&gt;Mask Vector&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;StarGAN은 mask vector m을 통해서 필요없는 label은 무시하고 특정 dataset의 label에 집중하도록 도와준다. mask vector m을 나타내기 위해서 datasets의 수(data의 수가 아님) n개가 있다면 &lt;strong&gt;n-dimensional one-hot vector&lt;/strong&gt;을 구성한다. 
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/10.png&quot; alt=&quot;&quot; width=&quot;40%&quot; height=&quot;40%&quot; class=&quot;center&quot; /&gt;&lt;br /&gt;
C&lt;sub&gt;i&lt;/sub&gt;는 i번째 dataset의 label을 나타내는 vector이다. label c&lt;sub&gt;i&lt;/sub&gt;를 표현하는 방법은 binary attribute의 &lt;strong&gt;binary vector&lt;/strong&gt; 혹은 categorical attribute을 나타내는 &lt;strong&gt;one-hot vector&lt;/strong&gt;일 수 있다. 이 때 나머지 n-1개의 unknown labels는 모두 0으로 준다.&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Training Strategy&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  위에 설명드렸던 domain label vector c ̃를 generator의 input으로 넣을 것이다. 그러면 generator은 불필요한 label(zero vector)을 무시하고 주어진 label을 더욱 집중할 수 있을 것이다..&lt;br /&gt;
&lt;b&gt;
“By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label.
“
&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;G의 구조는 input label c ̃의 차원이 아닌 single dataset 학습 할 때와 동일하다.&lt;/li&gt;
  &lt;li&gt;모든 dataset에 대한 label에 대해 probability distribution을 만들기 위해 D의 auxiliary classfier를 확장한다.&lt;/li&gt;
  &lt;li&gt;D가 인식 된 label과 관련된 classification error만을 최소화시키기 위해 multi-task에서 학습을 수행한다.
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;4. Implementation&lt;/h2&gt;
&lt;h4&gt;&lt;b&gt;Improved GAN Training&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  학습을 안정화시키고 high quality image를 만들기 위해 Adversarial loss를 gradient penalty가 포함 된 &lt;strong&gt;Wasserstein GAN objective&lt;/strong&gt;로 바꾼다.
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/05.png&quot; alt=&quot;&quot; width=&quot;60%&quot; height=&quot;60%&quot; class=&quot;center&quot; /&gt;&lt;/p&gt;
&lt;center&gt;↓&lt;/center&gt;
&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/11.png&quot; alt=&quot;&quot; width=&quot;60%&quot; height=&quot;60%&quot; class=&quot;center&quot; /&gt;&lt;br /&gt;
λ&lt;sub&gt;gp&lt;/sub&gt; = 10으로 설정한다. x^는 한 pair의 real image와 생성된 이미지 사이의 직선을 따라 균일하게 샘플링 된다.(?)&lt;br /&gt;&lt;/p&gt;

&lt;h4&gt;&lt;b&gt;Network Architecture&lt;/b&gt;&lt;/h4&gt;
&lt;p&gt;  StarGaN의 generator network와 discriminator는 다음과 같이 구성되어 있다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Generator network architecture
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/12.png&quot; alt=&quot;&quot; width=&quot;80%&quot; height=&quot;80%&quot; class=&quot;center&quot; /&gt;&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Discriminator network architecture
&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/13.png&quot; alt=&quot;&quot; width=&quot;80%&quot; height=&quot;80%&quot; class=&quot;center&quot; /&gt;
&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;5. Experiments&lt;/h2&gt;

&lt;h3&gt;5.1. Baseline Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DIAT&lt;/li&gt;
  &lt;li&gt;CycleGAN&lt;/li&gt;
  &lt;li&gt;IcGAN&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;5.2. Dataset&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CelebA : 202,599 face images of celebrities, 40 binary attributes, 7 domains
    &lt;ul&gt;
      &lt;li&gt;Attributes : hair color(black, blond, brown), gender(male/female), age(young/old)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RaFD : 4,824 images collected from 67 participants, 8 facial expression in 3 different gaze directions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;5.3. Training&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Using Adam optimizer, β&lt;sub&gt;1&lt;/sub&gt; = 0.5, β&lt;sub&gt;2&lt;/sub&gt; = 0.999&lt;/li&gt;
  &lt;li&gt;Batch size : 16&lt;/li&gt;
  &lt;li&gt;Learning rate
    &lt;ul&gt;
      &lt;li&gt;CelebA : 0.0001에서 10 epochs마다 감소&lt;/li&gt;
      &lt;li&gt;RaFD : 0.0001에서 100 epochs 마다 감소&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Source : single NVIDIA Tesla M40 GPU&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;5.4. Experimental Results on CelebA&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/16.png&quot; alt=&quot;&quot; class=&quot;center&quot; /&gt;
  먼저 CelebA로만 학습시켰을 때의 결과다. multiple attribute에서 합성을 진행하였으며, 좋은 퀄리티를 낼 수 있었다.&lt;/p&gt;

&lt;h3&gt;5.5. Experimental Results on RaFD&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/17.png&quot; alt=&quot;&quot; class=&quot;center&quot; /&gt;
  다음은 RaFD로만 학습을 시켰을 때의 결과이다.&lt;/p&gt;

&lt;h3&gt;5.6. Experimental Results on CelebA + RaFD&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;http://localhost:4000/assets/img/StarGAN/18.png&quot; alt=&quot;&quot; class=&quot;center&quot; /&gt;
  Multi Domatin뿐만 아니라 Multi dataset으로도 학습시킨 결과이다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;StarGAN(SNG) : RaFD로 학습시킨 모델로 CelebA에 적용시킨 결과&lt;/li&gt;
  &lt;li&gt;StarGAN(JNT) : RaFD + CelebA로 학습시킨 모델로 CelebA에 적용시킨 결과&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;multi dataset으로 학습시킨 모델이 조금 더 사진을 잘 생성해내는 것을 알 수 있다.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>pirunita</name>
          
          
        </author>
      

      

      
        <category term="DeepLearning" />
      
        <category term="GAN" />
      

      
        <summary type="html">StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Choi, Yunjey, et al. “Stargan: Unified generative adversarial networks for multi-domain image-to-image translation.” arXiv preprint 1711 (2017). 1. Introduction   image-to-image translation은 한 이미지에서 다른 이미지로 바꾸는 기술이다. 특히 Generative Adversarial Networks(GANs)로 image-to-image translation의 기술을 크게 발전시켰다. 두 개의 서로 다른 domain으로부터 training data가 주어졌을 때 model은 한 domain에서 다른 domain으로 image translation을 하도록 학습한다. 이 때의 term들을 다음과 같이 정의한다. attribute : meaningful feature(hair color, gender, age..) attribute value : a particular value of an attribute(brown, black, male, female..) domain : a set of images sharing the same attribute 특히 몇몇 image datasets은 수많은 labeled attributes와 관련이 있다. CelebA dataset : facial attributes(머리색, 성별, 나이 등..)와 관련된 40개의 label들을 가지고 있다. RaFD dataset : facial expressions(happy, angry, sad)와 같은 8개의 label들을 가지고 있다. StarGAN은 이러한 점에서 multiple domain으로부터 attribute에 따라 image를 바꾸는 multi-domain image-to-image translation을 생각해낸 것이다. Figure 1의 우측은 RaFD를 학습하면서 얻은 feature를 이용하여 CelebA와 RaFD jointly training으로 CelebA image의 표정을 바꾸는 학습을 진행한다.   그러나 기존의 모델은 multi-domain image translation에 매우 비효율적이다. 그 이유는 다음 3가지와 같다. k개의 domain 간 mapping을 학습할 때 (k-1)개의 generator를 학습해야 하기 때문이다. 또한 각각의 generator들은 전체 training data를 완전히 사용하지 못하고 k개의 domain 중에 2개만 학습한다. 이는 생성되는 이미지의 품질 저하를 일으킨다. 게다가 기존의 모델에서는 서로 다른 dataset으로부터 jointly training domain이 불가능하다. 왜냐하면 각각의 dataset은 partially labeled이기 때문에..Section 3.2   StarGAN은 위의 그림처럼 하나의 generator로 여러 multiple domain 사이의 mapping을 학습시키는 모델 구조를 제안한다. 또한 input으로는 image와 domain information을 넣는다. 이 때 domain information은 label(binary나 one-hot vector)을 사용한다. 그리고 서로 다른 dataset의 domain의 joint training을 위해 domain label에 mask vector라는 정보를 추가한다. 이 방법은 알려지지 않는 label은 무시하고 특정 dataset의 label에만 집중 할 수 있게 된다. 따라서 요악하면 다음과 같다. 각각의 domain의 image로부터 효율적으로 image-to-image translation을 학습하기 위해 single generator와 discriminator을 이용하여 multiple domains의 mapping을 학습하는 starGAN 제안 domain labels를 control하기 위해 mask vector를 사용하여 multi domain image translation 학습 StarGAN을 이용하여 facial attribute transfer와 facial expression synthesis에서 좋은 성과를 얻음 2. Related Work Generative Adversarial Networks 생성되는 이미지가 더욱 실감나도록 adversarial loss를 leverage Conditional GANs 본 논문에서는 conditional domain information을 넣기 위해 scalabe GAN framework를 사용한다. 이는 다양한 domain에서 image translation을 유연하게 control할 수 있도록 한다. Image-to-Image Translation CycleGAN과 DiscoGAN은 cycle consistency loss를 활용하여 input과 translated image 사이의 key attributes를 보존하므로 즉, 복원된 이미지가 입력 이미지와 비슷하게 학습시키기 위해 starGAN에서도 cycle consistency loss를 추가한다. 3. Star Generative Adversarial Networks 이번 섹션에서는 서로 다른 label을 갖는 multiple dataset을 가지고 StarGAN으로 label을 통해 image translation 시키는 지 알아본다. 3.1. Multi-Domain Image-to-Image Translation   먼저 Generator를 학습시키기 위해서 input: x, target domain label: c, output: y G(x, c) → y 또한 auxiliary classifier을 통해 하나의 discriminator가 multiple domain을 control할 수 있도록 하였다. Discrimintor은 source와 domain label의 probability distributions을 생성한다. D : x → {Dsrc(x), Dcls(x)} Adversarial Loss   생성된 이미지와 원본 이미지를 구별하기 위해 adversarial loss를 사용한다. 여기서 Dsrc(x)는 discriminator D에 의해 생성된 source의 probability distribution을 뜻한다. Generator G는 최소화되고 discriminator D는 최대화하도록 할 것이다. Domain Classification Loss   input x와 target domain label c가 주어졌을 때 우리는 x에서 target domain c로 적절히 분리 된 output y를 만드는 것이다. 이를 위해서 D의 상단에 auxiliary classifier을 추가하고 D와 G를 최적화 할 때 domain classification loss를 추가한다. 정리하면 다음과 같다. 실제 이미지의 domain classification loss → D를 최적화 가짜 이미지의 domain classification loss → G를 최적화 여기서 Dcls(c’ㅣx)는 D에 의해 계산된 domain label의 probability distribution이다. 따라서 D는 결국 이 classification loss를 최소화시켜, original domain c’에 해당하는 real image를 분류할 수 있다. 반면에 fake image의 domain classification에 대한 loss funcion은 다음과 같다. 여기서 G는 위 식의 loss를 줄여나가며 target domain c에 분류되는 이미지를 생성하도록 한다. Reconstruction Loss   위의 설명했던 adversarial loss와 classification losses를 최소화함으로써 G는 더 현실적인 이미지를 생성하도록 학습하고 target domain에 정확히 대응되도록 분류한다. “However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs.” 하지만 loss들을 최소화하면 바뀐 이미지가 input과 관련된 domain을 바꾸는 동안 입력 이미지의 내용을 유지한다는 보장이 없다.(?) 따라서 generator에 cycle consistency loss를 적용시켜 문제를 해결하고자 했다. 이는 다음과 같다. 여기서 G는 변환이미지 G(x,c)와 original domain label c’를 input으로 다시 original image x를 재구축(reconstruction)하게 되며 L1 norm을 reconstruction loss로 사용하였다. 이 때 주목할 점은 single generator을 두 번 사용한다 그렇다면 이제 다음 그림을 보면서 StarGAN의 전반적인 학습 프로세스를 요악한다. G(x:input image, c:target domain) → y:Fake image G(y:Fake image, c’:original domain) → x:reconstructed image D : Distinguish between real &amp;amp; fake + auxiliary classifier(domain label classification) Full Objective   G와 D를 최적화하는 objective functions는 다음과 같다. 3.2. Training with Multiple Datasets   StarGAN의 중요한 장점은 전혀 다른 label을 포함한 multiple dataset을 동시에 적용시켜 모든 label을 control할 수 있다는 점이다. 앞서 설명드린 서로 다른 label을 갖는 CelebA와 RaFD의 경우다. 예를 들어 CelebA는 attribute를 갖지만 facial expression label은 갖고 있지 않다. 이를 해결하기 위해서 reconstruction process의 label vector c’를 추가할 것이다.. Mask Vector StarGAN은 mask vector m을 통해서 필요없는 label은 무시하고 특정 dataset의 label에 집중하도록 도와준다. mask vector m을 나타내기 위해서 datasets의 수(data의 수가 아님) n개가 있다면 n-dimensional one-hot vector을 구성한다. Ci는 i번째 dataset의 label을 나타내는 vector이다. label ci를 표현하는 방법은 binary attribute의 binary vector 혹은 categorical attribute을 나타내는 one-hot vector일 수 있다. 이 때 나머지 n-1개의 unknown labels는 모두 0으로 준다. Training Strategy   위에 설명드렸던 domain label vector c ̃를 generator의 input으로 넣을 것이다. 그러면 generator은 불필요한 label(zero vector)을 무시하고 주어진 label을 더욱 집중할 수 있을 것이다.. “By doing so, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label. “ G의 구조는 input label c ̃의 차원이 아닌 single dataset 학습 할 때와 동일하다. 모든 dataset에 대한 label에 대해 probability distribution을 만들기 위해 D의 auxiliary classfier를 확장한다. D가 인식 된 label과 관련된 classification error만을 최소화시키기 위해 multi-task에서 학습을 수행한다. 4. Implementation Improved GAN Training   학습을 안정화시키고 high quality image를 만들기 위해 Adversarial loss를 gradient penalty가 포함 된 Wasserstein GAN objective로 바꾼다. ↓ λgp = 10으로 설정한다. x^는 한 pair의 real image와 생성된 이미지 사이의 직선을 따라 균일하게 샘플링 된다.(?) Network Architecture   StarGaN의 generator network와 discriminator는 다음과 같이 구성되어 있다. Generator network architecture Discriminator network architecture 5. Experiments 5.1. Baseline Models DIAT CycleGAN IcGAN 5.2. Dataset CelebA : 202,599 face images of celebrities, 40 binary attributes, 7 domains Attributes : hair color(black, blond, brown), gender(male/female), age(young/old) RaFD : 4,824 images collected from 67 participants, 8 facial expression in 3 different gaze directions 5.3. Training Using Adam optimizer, β1 = 0.5, β2 = 0.999 Batch size : 16 Learning rate CelebA : 0.0001에서 10 epochs마다 감소 RaFD : 0.0001에서 100 epochs 마다 감소 Source : single NVIDIA Tesla M40 GPU 5.4. Experimental Results on CelebA   먼저 CelebA로만 학습시켰을 때의 결과다. multiple attribute에서 합성을 진행하였으며, 좋은 퀄리티를 낼 수 있었다. 5.5. Experimental Results on RaFD   다음은 RaFD로만 학습을 시켰을 때의 결과이다. 5.6. Experimental Results on CelebA + RaFD   Multi Domatin뿐만 아니라 Multi dataset으로도 학습시킨 결과이다. StarGAN(SNG) : RaFD로 학습시킨 모델로 CelebA에 적용시킨 결과 StarGAN(JNT) : RaFD + CelebA로 학습시킨 모델로 CelebA에 적용시킨 결과 multi dataset으로 학습시킨 모델이 조금 더 사진을 잘 생성해내는 것을 알 수 있다.</summary>
      

      
      
    </entry>
  
  
  
    <entry xml:lang="ko">
      
      <title type="html">Category test</title>
      
      
      <link href="http://localhost:4000/2018/12/06/category-test/" rel="alternate" type="text/html" title="Category test" />
      
      <published>2018-12-06T18:07:17+00:00</published>
      <updated>2018-12-06T18:07:17+00:00</updated>
      <id>http://localhost:4000/2018/12/06/category%20test</id>
      <content type="html" xml:base="http://localhost:4000/2018/12/06/category-test/">&lt;p&gt;category test
카테고리 테스트입니다.&lt;/p&gt;</content>

      
      
      
      
      

      
        <author>
            <name>pirunita</name>
          
          
        </author>
      

      

      
        <category term="DeepLearning" />
      

      
        <summary type="html">category test 카테고리 테스트입니다.</summary>
      

      
      
    </entry>
  
  
</feed>
