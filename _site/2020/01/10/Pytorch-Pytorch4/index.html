<!DOCTYPE html>
<html lang="ko">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>[PyTorch] 4. It starts with a tensor(3)</title>
  <meta name="description" content="PyTorch 4. Numeric types   Tensor도 numpy array의 dtype argument처럼 numerical data type을 가지게 된다. torch dtype numerical data type torch.float32 or torch.float 32-bit, floating-point torch.float64 or torch.double 64-bit, double_precision floating-point torch.float16 or torch.half 16-bit, half-preicision floating-point torch.int8 Signed 8-bit integers torch.int16 or torch.short Signed 16-bit integers torch.int32 or torch.int Signed 32-bit integers torch.int64 or torch.long Signed 64-bit integers tensor에 적절한 numeric type을 부여하기 위해 argument로 dtype을 넣어 정한다. 또한 dtype에 대해 찾으려면 예시는 다음과 같다. double_points = torch.ones(10, 2, type=torch.double) short_points = torch.tensor([[1, 2], [3, 4]], dtype=torch.short) short_points.dtype &amp;gt;&amp;gt;&amp;gt; torch.int16 또한 casting method를 이용하여 적절한 type으로 casting하는 다양한 tensor-creation function을 쓸 수 있다. method로는 to, type을 사용할 수 있다. double_points = torch.zeros(10, 2).double() short_points = torch.ones(10, 2).short() double_points = torch.zeros(10, 2).to(torch.double) short_points = torch.ones(10, 2).to(dtype=torch.short) points = torch.randn(10, 2) short_points = points.type(torch.short) 5. Indexing tensors   Tensor는 python의 기본 list처럼 index들을 slicing 할 수 있다. range를 사용함으로써 PyTorch도 강력한 indexing을 가지며 이를 advanced indexing이라고 한다. # list some_list = list(range(6)) some_list[:] # All elements in the list some_list[1:4] # From element 1 inclusive to element 4 exclusive some_list[1:] # From element 1 inclusive to the end of the list some_list[:4] # From the start of the list to element 4 exclusive some_list[:-1] # From the start of the list to one before the last element some_list[1:4:2] # From element 1 inclusive to element 4 exclusive in steps of 2 # Tensor points[1:] # All rows after first, implicitly all columns points[1:, :] # All rows after first, all columns points[1:, 0] # All rows after first, first column 6. Numpy interoperability   PyTorch의 tensor는 Numpy array로 변환될 수 있어서 Numpy 배열을 중심으로 구축된 더 넓은 Python ecosystem의 방대한 기능들을 효율적으로 사용할 수 있다. 그리고 Numpy array는 Python buffer protocol이 작동하는 storage system덕분에 zero-copy interoperability가 가능하다. points = torch.ones(3, 4) points_np = points.numpy() points_np &amp;gt;&amp;gt;&amp;gt; array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], dtype=float32) 다음과 같이 같은 size, shape, numerical type의 Numpy 다차원 배열을 반환하며, 이 때 반환되는 array는 tensor storage와 기본 buffer를 공유한다. 즉, numpy method는 CPU RAM에서 cost가 꼭 소모되지 않고도 Numpy array에서 originating tensor로 바꿀 수 있다. 또한 GPU에 할당된 tensor가 있다면, PyTorch는 CPU에 할당되는 Numpy array로 복사하게 된다. PyTorch tensor에서 Numpy array로 바꾸는 것은 다음과 같다. points = torch.from_numpy(points_np) 7. Seriallizing tensors   PyTorch는 tensor object를 직렬화 하기 위해 pickle을 사용하고, storage에 serialization 코드를 사용한다. tensor를 저장 해 보자! &quot;&quot;&quot; Saving points &quot;&quot;&quot; torch.save(points, &#39;../data/ourpoints.t&#39;) # or with open(&#39;./data/ourpoints.t&#39;, &#39;wb&#39;) as f: torch.save(points. f) &quot;&quot;&quot; Loading points back &quot;&quot;&quot; points = torch.load(&#39;./data/ourpoints.t&#39;) # or with open(&#39;./data/ourpoitns.t&#39;, &#39;rb&#39;) as f: points = torch.load(f) 이러한 테크닉은 tensor를 저장하고 원할 때 PyTorch에서 불러올 수 있게 되지만 프로젝트에선 자주 사용되지는 않는다. 하지만 PyTorch를 이미 다른 library에 의존하는 시스템에 도입하려 할 때 사용될 수 있다. 직렬화된 다차원 행렬을 지원하는 HDF5 format과 library를 사용하는 예를 들어보자. import h5py f = h5py.File(&#39;./data/ourpoints.hdf5&#39;, &#39;w&#39;) dataset = f.create_dataset(&#39;coords&#39;, data=points.numpy()) f.close() 여기서 ‘coords’는 HDF5 file의 key 이다. 여기서 우리는 dataset을 index할 수 있으며 우리가 원하는 element에 only-access가 가능하다. f = h5py.File(&#39;./data/ourpoints.hdf5&#39;, &#39;r&#39;) dataset = f[&#39;coords&#39;] last_points = dataset[1:] 여기서는 파일을 열거나 Dataset이 필요할 때 Data가 로드되지 않고, dataset[1:]을 요청할 때까지 데이터는 디스크에 남아 있었다. 이 때 h5py는 이 두 column에 access해서 Numpy array처럼 동작하고 동일한 API를 갖는 해당 dataset의 영역을 encapsulation하는 Numpy 배열의 유사 객체를 반환한다. 반환된 객체는 torch.from_numpy 함수에 전달하여 tensor를 얻을 수 있고 Data는 tensor storage로 복사된다. last_points = torch.from_numpy(dataset[1:]) f.close() Eli Stevens, Luca Antiga. Deep Learning with PyTorch">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2020/01/10/Pytorch-Pytorch4/">
  
  
  <link rel="alternate" type="application/rss+xml" title="pirunita" href="http://localhost:4000/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="[PyTorch] 4. It starts with a tensor(3)">
  <meta name="twitter:description" content="PyTorch 4. Numeric types   Tensor도 numpy array의 dtype argument처럼 numerical data type을 가지게 된다. torch dtype numerical data type torch.float32 or torch.float 32-bit, floating-point torch.float64 or to...">
  
  
  
    





  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    
    <a class="site-title" href="/">
      <img src="/assets/main.jpeg", height="15%", width="15%">
      pirunita
    </a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/tagcloud/">TagCloud</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
    
      <h1 class="post-title" itemprop="name headline">[PyTorch] 4. It starts with a tensor(3)</h1>
    
    <p class="post-meta"><time datetime="2020-01-10T03:00:00+00:00" itemprop="datePublished">Jan 10, 2020</time>
      <span>
        
          
          <a href="/tag/DeepLearning"><code class="highligher-rouge"><nobr>DeepLearning</nobr></code>&nbsp;</a>
        
      </span>
    </p>
  </header>
  
  <div class="post-content" itemprop="articleBody">
    <hr />

<h2>PyTorch</h2>

<h3>4. Numeric types</h3>
<p>  Tensor도 numpy array의 dtype argument처럼 numerical data type을 가지게 된다.</p>

<table>
  <thead>
    <tr>
      <th>torch dtype</th>
      <th>numerical data type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torch.float32 or torch.float</td>
      <td>32-bit, floating-point</td>
    </tr>
    <tr>
      <td>torch.float64 or torch.double</td>
      <td>64-bit, double_precision floating-point</td>
    </tr>
    <tr>
      <td>torch.float16 or torch.half</td>
      <td>16-bit, half-preicision floating-point</td>
    </tr>
    <tr>
      <td>torch.int8</td>
      <td>Signed 8-bit integers</td>
    </tr>
    <tr>
      <td>torch.int16 or torch.short</td>
      <td>Signed 16-bit integers</td>
    </tr>
    <tr>
      <td>torch.int32 or torch.int</td>
      <td>Signed 32-bit integers</td>
    </tr>
    <tr>
      <td>torch.int64 or torch.long</td>
      <td>Signed 64-bit integers</td>
    </tr>
  </tbody>
</table>

<p>tensor에 적절한 numeric type을 부여하기 위해 argument로 <strong>dtype</strong>을 넣어 정한다. 또한 dtype에 대해 찾으려면 예시는 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">double_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">short_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">short</span><span class="p">)</span>
<span class="n">short_points</span><span class="o">.</span><span class="n">dtype</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt;
torch.int16
</code></pre></div></div>

<p>또한 <strong>casting method</strong>를 이용하여 적절한 type으로 casting하는 다양한 <strong>tensor-creation function</strong>을 쓸 수 있다. method로는 <strong>to</strong>, <strong>type</strong>을 사용할 수 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">double_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">double</span><span class="p">()</span>
<span class="n">short_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">short</span><span class="p">()</span>

<span class="n">double_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="n">short_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">short</span><span class="p">)</span>

<span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">short_points</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">short</span><span class="p">)</span>
</code></pre></div></div>

<h3>5. Indexing tensors</h3>
<p>  Tensor는 python의 기본 list처럼 index들을 <strong>slicing</strong> 할 수 있다. range를 사용함으로써 PyTorch도 강력한 indexing을 가지며 이를 <strong>advanced indexing</strong>이라고 한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># list
</span><span class="n">some_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">6</span><span class="p">))</span>
<span class="n">some_list</span><span class="p">[:]</span>            <span class="c1"># All elements in the list
</span><span class="n">some_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>          <span class="c1"># From element 1 inclusive to element 4 exclusive
</span><span class="n">some_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>           <span class="c1"># From element 1 inclusive to the end of the list
</span><span class="n">some_list</span><span class="p">[:</span><span class="mi">4</span><span class="p">]</span>           <span class="c1"># From the start of the list to element 4 exclusive
</span><span class="n">some_list</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>          <span class="c1"># From the start of the list to one before the last element
</span><span class="n">some_list</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="mi">4</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span>        <span class="c1"># From element 1 inclusive to element 4 exclusive in steps of 2
</span>
<span class="c1"># Tensor
</span><span class="n">points</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>              <span class="c1"># All rows after first, implicitly all columns
</span><span class="n">points</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="p">:]</span>           <span class="c1"># All rows after first, all columns
</span><span class="n">points</span><span class="p">[</span><span class="mi">1</span><span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>           <span class="c1"># All rows after first, first column
</span></code></pre></div></div>

<h3>6. Numpy interoperability</h3>
<p>  PyTorch의 tensor는 Numpy array로 변환될 수 있어서 Numpy 배열을 중심으로 구축된 더 넓은 Python ecosystem의 방대한 기능들을 효율적으로 사용할 수 있다. 그리고 Numpy array는 Python buffer protocol이 작동하는 storage system덕분에 <strong>zero-copy interoperability</strong>가 가능하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">points_np</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">points_np</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt;
array([[1., 1., 1., 1.],
       [1., 1., 1., 1.],
       [1., 1., 1., 1.]], dtype=float32)
</code></pre></div></div>

<p>다음과 같이 같은 size, shape, numerical type의 Numpy 다차원 배열을 반환하며, <em>이 때 반환되는 array는 tensor storage와 기본 buffer를 공유한다.</em> 즉, <strong>numpy method</strong>는 CPU RAM에서 cost가 꼭 소모되지 않고도 Numpy array에서 originating tensor로 바꿀 수 있다. 또한 GPU에 할당된 tensor가 있다면, PyTorch는 CPU에 할당되는 Numpy array로 복사하게 된다.</p>

<p>PyTorch tensor에서 Numpy array로 바꾸는 것은 다음과 같다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">points_np</span><span class="p">)</span>
</code></pre></div></div>

<h3>7. Seriallizing tensors</h3>
<p>  PyTorch는 tensor object를 직렬화 하기 위해 <strong>pickle</strong>을 사용하고, storage에 serialization 코드를 사용한다. tensor를 저장 해 보자!</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" Saving points """</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="s">'../data/ourpoints.t'</span><span class="p">)</span>

<span class="c1"># or
</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'./data/ourpoints.t'</span><span class="p">,</span> <span class="s">'wb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">points</span><span class="o">.</span> <span class="n">f</span><span class="p">)</span>

<span class="s">""" Loading points back """</span>
<span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">'./data/ourpoints.t'</span><span class="p">)</span>

<span class="c1"># or
</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'./data/ourpoitns.t'</span><span class="p">,</span> <span class="s">'rb'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
</code></pre></div></div>

<p>이러한 테크닉은 tensor를 저장하고 원할 때 PyTorch에서 불러올 수 있게 되지만 프로젝트에선 자주 사용되지는 않는다. 하지만 PyTorch를 이미 다른 library에 의존하는 시스템에 도입하려 할 때 사용될 수 있다. 직렬화된 다차원 행렬을 지원하는 HDF5 format과 library를 사용하는 예를 들어보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">h5py</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="s">'./data/ourpoints.hdf5'</span><span class="p">,</span> <span class="s">'w'</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">create_dataset</span><span class="p">(</span><span class="s">'coords'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">points</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div></div>

<p>여기서 <strong>‘coords’</strong>는 HDF5 file의 <strong>key</strong> 이다. 여기서 우리는 dataset을 index할 수 있으며 우리가 원하는 element에 <strong>only-access</strong>가 가능하다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span> <span class="o">=</span> <span class="n">h5py</span><span class="o">.</span><span class="n">File</span><span class="p">(</span><span class="s">'./data/ourpoints.hdf5'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="s">'coords'</span><span class="p">]</span>
<span class="n">last_points</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
</code></pre></div></div>

<p>여기서는 파일을 열거나 Dataset이 필요할 때 Data가 로드되지 않고, dataset[1:]을 요청할 때까지 데이터는 디스크에 남아 있었다. 이 때 h5py는 이 두 column에 access해서 Numpy array처럼 동작하고 동일한 API를 갖는 해당 dataset의 영역을 encapsulation하는 <strong>Numpy 배열의 유사 객체를 반환</strong>한다.<br />
반환된 객체는 <strong>torch.from_numpy</strong> 함수에 전달하여 tensor를 얻을 수 있고 Data는 tensor storage로 복사된다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">last_points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">dataset</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
</code></pre></div></div>

<hr />

<p>Eli Stevens, Luca Antiga. Deep Learning with PyTorch</p>


  </div>
  <div class="post-comments" itemprop="comment">
    <hr />
<h1>Comments</h1>
<p>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pirunita.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</p>

  </div>
  
  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">
    <p>
      <!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

    
  
    
  
    
    <li><a href="mailto:khosungpil@gmail.com" class="icon-8 email" title="Email"><svg viewBox="0 0 512 512"><path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z"/></svg><!--[if lt IE 9]><em>Email</em><![endif]--></a></li>
    
  
    
  
    
    
    
    <li><a href="https://github.com/pirunita" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
    
  
    
  
    
    <li><a href="https://instagram.com/khosungpil" class="icon-15 instagram" title="Instagram"><svg viewBox="0 0 512 512"><g><path d="M256 109.3c47.8 0 53.4 0.2 72.3 1 17.4 0.8 26.9 3.7 33.2 6.2 8.4 3.2 14.3 7.1 20.6 13.4 6.3 6.3 10.1 12.2 13.4 20.6 2.5 6.3 5.4 15.8 6.2 33.2 0.9 18.9 1 24.5 1 72.3s-0.2 53.4-1 72.3c-0.8 17.4-3.7 26.9-6.2 33.2 -3.2 8.4-7.1 14.3-13.4 20.6 -6.3 6.3-12.2 10.1-20.6 13.4 -6.3 2.5-15.8 5.4-33.2 6.2 -18.9 0.9-24.5 1-72.3 1s-53.4-0.2-72.3-1c-17.4-0.8-26.9-3.7-33.2-6.2 -8.4-3.2-14.3-7.1-20.6-13.4 -6.3-6.3-10.1-12.2-13.4-20.6 -2.5-6.3-5.4-15.8-6.2-33.2 -0.9-18.9-1-24.5-1-72.3s0.2-53.4 1-72.3c0.8-17.4 3.7-26.9 6.2-33.2 3.2-8.4 7.1-14.3 13.4-20.6 6.3-6.3 12.2-10.1 20.6-13.4 6.3-2.5 15.8-5.4 33.2-6.2C202.6 109.5 208.2 109.3 256 109.3M256 77.1c-48.6 0-54.7 0.2-73.8 1.1 -19 0.9-32.1 3.9-43.4 8.3 -11.8 4.6-21.7 10.7-31.7 20.6 -9.9 9.9-16.1 19.9-20.6 31.7 -4.4 11.4-7.4 24.4-8.3 43.4 -0.9 19.1-1.1 25.2-1.1 73.8 0 48.6 0.2 54.7 1.1 73.8 0.9 19 3.9 32.1 8.3 43.4 4.6 11.8 10.7 21.7 20.6 31.7 9.9 9.9 19.9 16.1 31.7 20.6 11.4 4.4 24.4 7.4 43.4 8.3 19.1 0.9 25.2 1.1 73.8 1.1s54.7-0.2 73.8-1.1c19-0.9 32.1-3.9 43.4-8.3 11.8-4.6 21.7-10.7 31.7-20.6 9.9-9.9 16.1-19.9 20.6-31.7 4.4-11.4 7.4-24.4 8.3-43.4 0.9-19.1 1.1-25.2 1.1-73.8s-0.2-54.7-1.1-73.8c-0.9-19-3.9-32.1-8.3-43.4 -4.6-11.8-10.7-21.7-20.6-31.7 -9.9-9.9-19.9-16.1-31.7-20.6 -11.4-4.4-24.4-7.4-43.4-8.3C310.7 77.3 304.6 77.1 256 77.1L256 77.1z"/><path d="M256 164.1c-50.7 0-91.9 41.1-91.9 91.9s41.1 91.9 91.9 91.9 91.9-41.1 91.9-91.9S306.7 164.1 256 164.1zM256 315.6c-32.9 0-59.6-26.7-59.6-59.6s26.7-59.6 59.6-59.6 59.6 26.7 59.6 59.6S288.9 315.6 256 315.6z"/><circle cx="351.5" cy="160.5" r="21.5"/></g></svg><!--[if lt IE 9]><em>Instagram</em><![endif]--></a></li>
    
  
    
    <li><a href="https://www.linkedin.com/in/pirunita" class="icon-17 linkedin" title="LinkedIn"><svg viewBox="0 0 512 512"><path d="M186.4 142.4c0 19-15.3 34.5-34.2 34.5 -18.9 0-34.2-15.4-34.2-34.5 0-19 15.3-34.5 34.2-34.5C171.1 107.9 186.4 123.4 186.4 142.4zM181.4 201.3h-57.8V388.1h57.8V201.3zM273.8 201.3h-55.4V388.1h55.4c0 0 0-69.3 0-98 0-26.3 12.1-41.9 35.2-41.9 21.3 0 31.5 15 31.5 41.9 0 26.9 0 98 0 98h57.5c0 0 0-68.2 0-118.3 0-50-28.3-74.2-68-74.2 -39.6 0-56.3 30.9-56.3 30.9v-25.2H273.8z"/></svg><!--[if lt IE 9]><em>LinkedIn</em><![endif]--></a></li>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
  </ul>
    </p>
    
    <p>
      

&copy; pirunita - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
