<!DOCTYPE html>
<html lang="ko">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  
  
  <title>[PyTorch] 2. It starts with a tensor(1) - basic</title>
  <meta name="description" content="PyTorch   Network는 floating-point numbers를 이용하여 information을 다루기 때문에 network가 이해할 수 있는 적절한 표현으로 data를 encoding하고 다시 우리가 이해할 수 있는 표현으로 decoding하여 output을 내야 한다.   Data의 어떤 form에서 다른 form으로 변환하는 것은 Deep neural network에 의해 학습된다. 이는 연속된 intermediate representation이 진행되는 단계 사이에 data가 부분적으로 transform된다고 볼 수 있다. 예를 들어 image recognition에서 early representation으로는 edge detection, texture(fur)이 될 수 있고, Deeper representation에서는 더욱 복잡한 구조(눈, 코, 입)를 찾아낼 수 있다. 앞선 예시처럼 일반적으로 이러한 intermediate representations은 Input을 특징화하고, Data에서 구조를 파악하며 Neural network의 output을 묘사하게 된다.   Intermediate representation을 위해 Data를 적절하게 floating-point input으로 바꿀 필요가 있으며 PyTorch에서는 기본 자료구조로 tensor를 이용하게 된다. 물론 tensor는 NumPy, SciPy, Scikit-learn, Pandas와 같은 라이브러리에서도 사용된다. 1. Tensor fundamentals tensor는 torch의 function을 이용하여 생성할 수도 있다. 또한 임의로 텐서를 바꿀 수도 있다. 하지만 딥러닝에선 잘 안쓰인다. import torch a = torch.ones(3) a a[1] float(a[1]) a[2] = 2.0 a &amp;gt;&amp;gt;&amp;gt; tensor([1., 1., 1.]) tensor(1.) 1.0 tensor([1., 1., 2.]) 2차원 tensor도 만들어 보자 points = torch.tensor([1.0, 4.0], [2.0, 1.0], [3.0, 5.0]) points points.shape &amp;gt;&amp;gt;&amp;gt; tensor([1., 4.], [2., 1.], [3., 5.]) torch.Size([3, 2]) size를 이용하여 zeros나 ones로 tensor를 만들 수 있고, FloatTensor로 만들 수도 있다. points = torch.zeros(3, 2) points points = torch.FloatTensor([1.0, 4.0], [2.0, 1.0], [3.0, 5.0]) points points[0, 1] points[0] tensor([0., 0.], [0., 0.], [0., 0.]) tensor([1., 4.], [2., 1.], [3., 5.]) tensor(4.) tensor([1., 4.]) 위에서 output으로 나온 결과는 다른 tensor이지만 새롭게 메모리를 할당받은 것은 아니다. 만약 tensor가 엄청나게 커지면 process가 부족해 지기 때문이다. 다만 같은 data 내에서 다른 view를 갖게 되는 것이다. 2. Tensors and storages   Tensor 내에서 value들은 contiguous 메모리에 할당 되고 이는 torch.Storage 인스턴스에 의해 관리된다. storage는 float이나 int32의 타입을 포함하는 메모리 블럭과 같은 1차원 숫자 배열로 이루어져 있다. PyTorch Tensor는 offset, per-demension stride를 사용함으로써 storage에 indexing이 가능한 view라고 볼 수 있다.(Database의 view 말하는 건가?) Tensors are views over a Storage instance   Multiple tensor은 data마다 index가 다르게 되어 있더라도 같은 storage에 index하게 된다. 위의 그림처럼 같은 index에 있는 storage에 다르게 생긴 tensor로 참조할 수 있다. (그래서 원하는 모양으로 불러올 수 있다고 해서 DB의 view라고 설명한 듯 하다.) 메모리가 일단 한 번 할당하게 되면 Storage instance에 의해 관리되는 data의 크기에 상관없이 alternative tensor views를 만들어 빠르게 처리할 수 있다. .storage property를 사용하여 tensor가 저장되는 sotrage에 접근 해 보자. points = torch.tensor([1.0, 4.0], [2.0, 1.0], [3.0, 5.0]) points.storage() 1.0 4.0 2.0 1.0 3.0 5.0 [torch.FloatStorage of size 6] 즉 tensor가 3 rows, 2 cols로 만들었어도 storage에서는 size 6의 contiguous array로 저장되게 된다. 다시 말해서, tensor는 storage에 있는 tensor의 위치에서 index 쌍들을 번역하게 되는 것이다. points_storage = points.storage() points_storage[0] points_storage[1] 1.0 4.0 storage에서 중요한 것은 2D tensor의 storage를 index할 수 없다는 것이다. storage는 항상 one-dimensional로 있다. points = torch.tensor([1.0, 4.0], [2.0, 1.0], [3.0, 5.0]) points_storage = points.storage() points_storage[0] = 2.0 points tensor([[2,. 4.], [2., 1.], [3., 5.]]) 만약 위의 코드와 같이 storage instance를 직접 이용할 수도 있다. 이는 tensor와 storage 간의 관계를 이해하는 것이 추후에 특정 연산의 비용에 대한 이해도가 높아질 것이며 더 효율적인 PyTorch code를 작성하는 데 도움이 될 것이다! Eli Stevens, Luca Antiga. Deep Learning with PyTorch">
  

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/2020/01/05/Pytorch-Pytorch2/">
  
  
  <link rel="alternate" type="application/rss+xml" title="pirunita" href="http://localhost:4000/feed.xml">

  

  
  <meta name="twitter:card" content="summary">
  
  <meta name="twitter:title" content="[PyTorch] 2. It starts with a tensor(1) - basic">
  <meta name="twitter:description" content="PyTorch   Network는 floating-point numbers를 이용하여 information을 다루기 때문에 network가 이해할 수 있는 적절한 표현으로 data를 encoding하고 다시 우리가 이해할 수 있는 표현으로 decoding하여 output을 내야 한다.   Data의 어떤 form에서 다른 form으로 변환하는 것은 D...">
  
  
  
    





  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css?family=Bitter:400,400i,700" rel="stylesheet">

  

</head>


  <body>

    <header class="site-header">

  <div class="wrapper">
    
    <a class="site-title" href="/">
      <img src="/assets/main.jpeg", height="15%", width="15%">
      pirunita
    </a>
    <nav class="site-nav">
      
        
        <a class="page-link" href="/about/">About</a>
      
        
        <a class="page-link" href="/tagcloud/">TagCloud</a>
      
        
        <a class="page-link" href="/archives/">Archives</a>
      
    </nav>

  </div>

</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    
    
      <h1 class="post-title" itemprop="name headline">[PyTorch] 2. It starts with a tensor(1) - basic</h1>
    
    <p class="post-meta"><time datetime="2020-01-05T03:00:00+00:00" itemprop="datePublished">Jan 5, 2020</time>
      <span>
        
          
          <a href="/tag/DeepLearning"><code class="highligher-rouge"><nobr>DeepLearning</nobr></code>&nbsp;</a>
        
      </span>
    </p>
  </header>
  
  <div class="post-content" itemprop="articleBody">
    <hr />

<h2>PyTorch</h2>

<p>  Network는 <strong>floating-point numbers</strong>를 이용하여 information을 다루기 때문에 network가 이해할 수 있는 적절한 표현으로 data를 encoding하고 다시 우리가 이해할 수 있는 표현으로 decoding하여 output을 내야 한다.<br /></p>

<p>  Data의 어떤 form에서 다른 form으로 변환하는 것은 Deep neural network에 의해 학습된다. 이는 연속된 intermediate representation이 진행되는 단계 사이에 data가 부분적으로 transform된다고 볼 수 있다. <br />예를 들어 image recognition에서 early representation으로는 edge detection, texture(fur)이 될 수 있고, Deeper representation에서는 더욱 복잡한 구조(눈, 코, 입)를 찾아낼 수 있다.<br />
앞선 예시처럼 일반적으로 이러한 intermediate representations은 <u>Input을 특징화하고</u>, <u>Data에서 구조를 파악</u>하며 <u>Neural network의 output을 묘사</u>하게 된다.
<img src="http://localhost:4000/assets/img/pytorch/2_1.png" alt="" width="85%" height="85%" class="center" />
<br /><br />
  Intermediate representation을 위해 Data를 적절하게 floating-point input으로 바꿀 필요가 있으며 PyTorch에서는 기본 자료구조로 <strong>tensor</strong>를 이용하게 된다. 물론 tensor는 NumPy, SciPy, Scikit-learn, Pandas와 같은 라이브러리에서도 사용된다.</p>

<h3>1. Tensor fundamentals</h3>
<p>tensor는 torch의 function을 이용하여 생성할 수도 있다. 또한 임의로 텐서를 바꿀 수도 있다. 하지만 딥러닝에선 잘 안쓰인다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">a</span>
<span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

<span class="nb">float</span><span class="p">(</span><span class="n">a</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">a</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">a</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt;
tensor([1., 1., 1.])
tensor(1.)
1.0
tensor([1., 1., 2.])
</code></pre></div></div>

<p>2차원 tensor도 만들어 보자</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>
<span class="n">points</span>
<span class="n">points</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&gt;&gt;&gt; 
tensor([1., 4.], [2., 1.], [3., 5.])
torch.Size([3, 2])
</code></pre></div></div>

<p>size를 이용하여 <strong>zeros</strong>나 <strong>ones</strong>로 tensor를 만들 수 있고, <strong>FloatTensor</strong>로 만들 수도 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">points</span>
<span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>
<span class="n">points</span>
<span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">points</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0., 0.],
       [0., 0.],
       [0., 0.])

tensor([1., 4.],
       [2., 1.],
       [3., 5.])

tensor(4.)
tensor([1., 4.])
</code></pre></div></div>

<p>위에서 output으로 나온 결과는 <strong>다른 tensor</strong>이지만 새롭게 메모리를 할당받은 것은 아니다. 만약 tensor가 엄청나게 커지면 process가 부족해 지기 때문이다. 다만 같은 data 내에서 다른 <strong>view</strong>를 갖게 되는 것이다.</p>

<h3>2. Tensors and storages</h3>
<p>  Tensor 내에서 value들은 contiguous 메모리에 할당 되고 이는 <strong>torch.Storage</strong> 인스턴스에 의해 관리된다. <strong>storage</strong>는 float이나 int32의 타입을 포함하는 메모리 블럭과 같은 1차원 숫자 배열로 이루어져 있다. PyTorch Tensor는 offset, per-demension stride를 사용함으로써 storage에 indexing이 가능한 <strong>view</strong>라고 볼 수 있다.<em>(Database의 view 말하는 건가?)</em></p>

<ul>
  <li>Tensors are views over a Storage instance
<img src="http://localhost:4000/assets/img/pytorch/2_2.png" alt="" width="85%" height="85%" class="center" /></li>
</ul>

<p>  Multiple tensor은 data마다 index가 다르게 되어 있더라도 같은 storage에 index하게 된다. 위의 그림처럼 같은 index에 있는 storage에 다르게 생긴 tensor로 참조할 수 있다. <em>(그래서 원하는 모양으로 불러올 수 있다고 해서 DB의 view라고 설명한 듯 하다.)</em> 메모리가 일단 한 번 할당하게 되면 <strong>Storage instance</strong>에 의해 관리되는 data의 크기에 상관없이 alternative tensor views를 만들어 빠르게 처리할 수 있다.</p>

<p><strong>.storage</strong> property를 사용하여 tensor가 저장되는 sotrage에 접근 해 보자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>
<span class="n">points</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.0
4.0
2.0
1.0
3.0
5.0
[torch.FloatStorage of size 6]
</code></pre></div></div>

<p>즉 tensor가 3 rows, 2 cols로 만들었어도 storage에서는 size 6의 contiguous array로 저장되게 된다. 다시 말해서, tensor는 storage에 있는 tensor의 위치에서 index 쌍들을 번역하게 되는 것이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points_storage</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
<span class="n">points_storage</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">points_storage</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1.0
4.0
</code></pre></div></div>

<p>storage에서 중요한 것은 2D tensor의 storage를 index할 수 없다는 것이다. storage는 항상 <strong>one-dimensional</strong>로 있다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">points</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">])</span>
<span class="n">points_storage</span> <span class="o">=</span> <span class="n">points</span><span class="o">.</span><span class="n">storage</span><span class="p">()</span>
<span class="n">points_storage</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">2.0</span>
<span class="n">points</span>
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[2,. 4.],
        [2., 1.],
        [3., 5.]])
</code></pre></div></div>

<p>만약 위의 코드와 같이 storage instance를 직접 이용할 수도 있다. 이는 tensor와 storage 간의 관계를 이해하는 것이 추후에 특정 연산의 비용에 대한 이해도가 높아질 것이며 더 효율적인 PyTorch code를 작성하는 데 도움이 될 것이다!</p>

<hr />

<p>Eli Stevens, Luca Antiga. Deep Learning with PyTorch</p>


  </div>
  <div class="post-comments" itemprop="comment">
    <hr />
<h1>Comments</h1>
<p>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://pirunita.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</p>

  </div>
  
  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">
    <p>
      <!-- Refer to https://codepen.io/ruandre/pen/howFi -->
<ul class="svg-icon">

    
  
    
  
    
    <li><a href="mailto:khosungpil@gmail.com" class="icon-8 email" title="Email"><svg viewBox="0 0 512 512"><path d="M101.3 141.6v228.9h0.3 308.4 0.8V141.6H101.3zM375.7 167.8l-119.7 91.5 -119.6-91.5H375.7zM127.6 194.1l64.1 49.1 -64.1 64.1V194.1zM127.8 344.2l84.9-84.9 43.2 33.1 43-32.9 84.7 84.7L127.8 344.2 127.8 344.2zM384.4 307.8l-64.4-64.4 64.4-49.3V307.8z"/></svg><!--[if lt IE 9]><em>Email</em><![endif]--></a></li>
    
  
    
  
    
    
    
    <li><a href="https://github.com/pirunita" class="icon-13 github" title="GitHub"><svg viewBox="0 0 512 512"><path d="M256 70.7c-102.6 0-185.9 83.2-185.9 185.9 0 82.1 53.3 151.8 127.1 176.4 9.3 1.7 12.3-4 12.3-8.9V389.4c-51.7 11.3-62.5-21.9-62.5-21.9 -8.4-21.5-20.6-27.2-20.6-27.2 -16.9-11.5 1.3-11.3 1.3-11.3 18.7 1.3 28.5 19.2 28.5 19.2 16.6 28.4 43.5 20.2 54.1 15.4 1.7-12 6.5-20.2 11.8-24.9 -41.3-4.7-84.7-20.6-84.7-91.9 0-20.3 7.3-36.9 19.2-49.9 -1.9-4.7-8.3-23.6 1.8-49.2 0 0 15.6-5 51.1 19.1 14.8-4.1 30.7-6.2 46.5-6.3 15.8 0.1 31.7 2.1 46.6 6.3 35.5-24 51.1-19.1 51.1-19.1 10.1 25.6 3.8 44.5 1.8 49.2 11.9 13 19.1 29.6 19.1 49.9 0 71.4-43.5 87.1-84.9 91.7 6.7 5.8 12.8 17.1 12.8 34.4 0 24.9 0 44.9 0 51 0 4.9 3 10.7 12.4 8.9 73.8-24.6 127-94.3 127-176.4C441.9 153.9 358.6 70.7 256 70.7z"/></svg><!--[if lt IE 9]><em>GitHub</em><![endif]--></a></li>
    
  
    
  
    
    <li><a href="https://instagram.com/khosungpil" class="icon-15 instagram" title="Instagram"><svg viewBox="0 0 512 512"><g><path d="M256 109.3c47.8 0 53.4 0.2 72.3 1 17.4 0.8 26.9 3.7 33.2 6.2 8.4 3.2 14.3 7.1 20.6 13.4 6.3 6.3 10.1 12.2 13.4 20.6 2.5 6.3 5.4 15.8 6.2 33.2 0.9 18.9 1 24.5 1 72.3s-0.2 53.4-1 72.3c-0.8 17.4-3.7 26.9-6.2 33.2 -3.2 8.4-7.1 14.3-13.4 20.6 -6.3 6.3-12.2 10.1-20.6 13.4 -6.3 2.5-15.8 5.4-33.2 6.2 -18.9 0.9-24.5 1-72.3 1s-53.4-0.2-72.3-1c-17.4-0.8-26.9-3.7-33.2-6.2 -8.4-3.2-14.3-7.1-20.6-13.4 -6.3-6.3-10.1-12.2-13.4-20.6 -2.5-6.3-5.4-15.8-6.2-33.2 -0.9-18.9-1-24.5-1-72.3s0.2-53.4 1-72.3c0.8-17.4 3.7-26.9 6.2-33.2 3.2-8.4 7.1-14.3 13.4-20.6 6.3-6.3 12.2-10.1 20.6-13.4 6.3-2.5 15.8-5.4 33.2-6.2C202.6 109.5 208.2 109.3 256 109.3M256 77.1c-48.6 0-54.7 0.2-73.8 1.1 -19 0.9-32.1 3.9-43.4 8.3 -11.8 4.6-21.7 10.7-31.7 20.6 -9.9 9.9-16.1 19.9-20.6 31.7 -4.4 11.4-7.4 24.4-8.3 43.4 -0.9 19.1-1.1 25.2-1.1 73.8 0 48.6 0.2 54.7 1.1 73.8 0.9 19 3.9 32.1 8.3 43.4 4.6 11.8 10.7 21.7 20.6 31.7 9.9 9.9 19.9 16.1 31.7 20.6 11.4 4.4 24.4 7.4 43.4 8.3 19.1 0.9 25.2 1.1 73.8 1.1s54.7-0.2 73.8-1.1c19-0.9 32.1-3.9 43.4-8.3 11.8-4.6 21.7-10.7 31.7-20.6 9.9-9.9 16.1-19.9 20.6-31.7 4.4-11.4 7.4-24.4 8.3-43.4 0.9-19.1 1.1-25.2 1.1-73.8s-0.2-54.7-1.1-73.8c-0.9-19-3.9-32.1-8.3-43.4 -4.6-11.8-10.7-21.7-20.6-31.7 -9.9-9.9-19.9-16.1-31.7-20.6 -11.4-4.4-24.4-7.4-43.4-8.3C310.7 77.3 304.6 77.1 256 77.1L256 77.1z"/><path d="M256 164.1c-50.7 0-91.9 41.1-91.9 91.9s41.1 91.9 91.9 91.9 91.9-41.1 91.9-91.9S306.7 164.1 256 164.1zM256 315.6c-32.9 0-59.6-26.7-59.6-59.6s26.7-59.6 59.6-59.6 59.6 26.7 59.6 59.6S288.9 315.6 256 315.6z"/><circle cx="351.5" cy="160.5" r="21.5"/></g></svg><!--[if lt IE 9]><em>Instagram</em><![endif]--></a></li>
    
  
    
    <li><a href="https://www.linkedin.com/in/pirunita" class="icon-17 linkedin" title="LinkedIn"><svg viewBox="0 0 512 512"><path d="M186.4 142.4c0 19-15.3 34.5-34.2 34.5 -18.9 0-34.2-15.4-34.2-34.5 0-19 15.3-34.5 34.2-34.5C171.1 107.9 186.4 123.4 186.4 142.4zM181.4 201.3h-57.8V388.1h57.8V201.3zM273.8 201.3h-55.4V388.1h55.4c0 0 0-69.3 0-98 0-26.3 12.1-41.9 35.2-41.9 21.3 0 31.5 15 31.5 41.9 0 26.9 0 98 0 98h57.5c0 0 0-68.2 0-118.3 0-50-28.3-74.2-68-74.2 -39.6 0-56.3 30.9-56.3 30.9v-25.2H273.8z"/></svg><!--[if lt IE 9]><em>LinkedIn</em><![endif]--></a></li>
    
  
    
  
    
  
    
  
    
  
    
  
    
  
  </ul>
    </p>
    
    <p>
      

&copy; pirunita - Powered by <a href="https://jekyllrb.com">Jekyll</a> &amp; <a href="https://github.com/yous/whiteglass">whiteglass</a> - Subscribe via <a href="http://localhost:4000/feed.xml">RSS</a>

    </p>

  </div>

</footer>


  </body>

</html>
